{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559f7b43",
   "metadata": {},
   "source": [
    "### Discussion Week 6\n",
    "\n",
    "We will review an example that highlights the need of being proficient in xpath syntax, because we are not able to inspect the html using devtools. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3441d5f1",
   "metadata": {},
   "source": [
    "## Xpath Scraping Examples & Explanation\n",
    "\n",
    "\n",
    "```markdown\n",
    "# Discussion: XPath Fundamentals and Practical Scraping\n",
    "\n",
    "XPath (XML Path Language) is a query language for selecting nodes from an XML/HTML document. With XPath, we can precisely locate elements in a structured document based on tags, attributes, text, and hierarchy.\n",
    "\n",
    "In web scraping scenarios, especially when working with HTML documents, XPath offers powerful capabilities such as:\n",
    "- **Selecting elements by tag** (e.g., `//a`, `//table`, etc.)\n",
    "- **Selecting elements by attribute** (e.g., `//div[@class=\"content\"]`)\n",
    "- **Selecting elements containing specific text** (e.g., `//td[contains(text(), \"Genre\")]`)\n",
    "- **Navigating the tree structure** (using child, sibling, or ancestor axes, such as `parent::`, `following-sibling::`, `preceding-sibling::`, etc.)\n",
    "\n",
    "Often, the HTML you get via an HTTP library (e.g., `requests`) can differ from what you see in your browser, because many sites serve different HTML to mobile vs. desktop clients, or because scripts dynamically manipulate the DOM. This can make scraping challenging if you rely solely on DevTools to copy selectors from a rendered page. Below is an illustrative example using `requests` and `lxml` to scrape [imsdb.com](https://imsdb.com/).\n",
    "\n",
    "---\n",
    "\n",
    "## Scraping Genre Links Example\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import lxml.html as lx\n",
    "\n",
    "# Step 1: Retrieve the page's HTML\n",
    "result = requests.get('https://imsdb.com/')\n",
    "result.raise_for_status()  # Ensure no HTTP errors\n",
    "html_content = result.text\n",
    "\n",
    "# Step 2: Parse the HTML content\n",
    "html = lx.fromstring(html_content)\n",
    "\n",
    "# Step 3: (Demonstration) Trying to select a specific <table><tbody> might return empty\n",
    "# (because the structure is different from what we see in some device view)\n",
    "genre_table = html.xpath('//table/tbody')\n",
    "print(\"Attempt to find table/tbody:\", genre_table)  # Likely returns []\n",
    "\n",
    "# Step 4: Different HTML is served depending on viewport/device. In some views,\n",
    "# the 'Genres' section might appear as a table row with a specific <td> containing the text \"Genre\".\n",
    "# In other (mobile) views, it might not appear at all (or only as a script-generated dropdown).\n",
    "# Let's assume we have the \"desktop\" or large-viewport HTML.\n",
    "\n",
    "# One trick: look for the table row containing the cell with text \"Genre\" \n",
    "# (or partial match in case there's trailing whitespace like \"\\r\\n\").\n",
    "genres = html.xpath('//table[tr/td[contains(text(), \"Genre\")]]/tr//a/@href')\n",
    "print(\"Genre links found:\", genres)\n",
    "\n",
    "# Explanation:\n",
    "#  - //table[tr/td[contains(text(), \"Genre\")]]: find a <table> that has a <tr>-><td> containing \"Genre\"\n",
    "#  - /tr//a/@href: within that table, find all <a> elements inside <tr> and return their \"href\" attributes\n",
    "```\n",
    "\n",
    "In many real-world scenarios, you must carefully inspect the raw HTML returned by `requests` (rather than the rendered HTML in your browser) to craft XPath queries that match the actual structure you’re scraping.\n",
    "\n",
    "---\n",
    "\n",
    "## Scraping Script Date Example\n",
    "\n",
    "In another scenario, suppose we want to retrieve the movie release year from a page like:\n",
    "[Interstellar Script](https://imsdb.com/Movie%20Scripts/Interstellar%20Script.html).\n",
    "\n",
    "After inspecting the HTML (mindful it may differ between desktop and mobile), we note that the script date is found as text after a `<b>` element with the text `\"Script Date\"`. For example:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import lxml.html as lx\n",
    "\n",
    "url = 'https://imsdb.com/Movie%20Scripts/Interstellar%20Script.html'\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "html = lx.fromstring(response.text)\n",
    "\n",
    "# We'll extract all text from <td> elements within the table that has class=\"script-details\"\n",
    "script_details_texts = html.xpath('//table[@class=\"script-details\"]//td/text()')\n",
    "print(\"Script details (all text):\", script_details_texts)\n",
    "\n",
    "# If we specifically want the text node immediately following the <b> element that has text \"Script Date\":\n",
    "date_text = html.xpath('//b[text()=\"Script Date\"]/following-sibling::text()[1]')\n",
    "print(\"Raw script date text:\", date_text)\n",
    "\n",
    "# The returned text might contain additional words, whitespace, or punctuation.\n",
    "# Next, we'd typically use regular expressions to isolate the four-digit year from the text.\n",
    "# For now, we'll just demonstrate that the immediate text is captured.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways on XPath Usage\n",
    "\n",
    "1. **Absolute vs. Relative Paths**  \n",
    "   - `//tag` searches for `<tag>` anywhere in the document, while `/tag` searches only in the immediate children of the current node.\n",
    "\n",
    "2. **Attribute Conditions**  \n",
    "   - `//div[@class=\"nav\"]` selects `<div>` elements with `class=\"nav\"`.\n",
    "\n",
    "3. **Text Matching**  \n",
    "   - `//td[text()=\"Genre\"]` matches a `<td>` whose **entire** text content is `\"Genre\"`.\n",
    "   - `//td[contains(text(),\"Genre\")]` matches a `<td>` whose text content contains `\"Genre\"` as a substring.\n",
    "\n",
    "4. **Handling Whitespace & Newlines**  \n",
    "   - Real HTML often includes line breaks like `\\r\\n`. To handle partial matches, use `contains()` or normalize space if needed.\n",
    "\n",
    "5. **Navigation Axes**  \n",
    "   - `following-sibling::`, `preceding-sibling::`, `parent::`, `child::`, etc. let you move in the document relative to a known node.\n",
    "\n",
    "By understanding these XPath strategies, you can more flexibly navigate HTML structures that aren’t always consistent — especially when the site provides different renders (e.g., mobile vs. desktop) or dynamically alters the DOM via JavaScript.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:**  \n",
    "To handle tricky situations where the page is significantly different when rendered in a browser (due to JavaScript or device-based rendering), you may need to:\n",
    "- Emulate a specific User-Agent and send the correct headers to get the “desktop” version.\n",
    "- Use a headless browser solution (e.g., `Selenium`, `Playwright`) to execute JavaScript and get the fully rendered page.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a8370",
   "metadata": {},
   "source": [
    "## Extended XPath Overview & Examples\n",
    "\n",
    "\n",
    "```markdown\n",
    "# Key Takeaways on XPath Usage\n",
    "\n",
    "## Absolute vs. Relative Paths\n",
    "- `//tag` searches for `<tag>` anywhere in the document.\n",
    "- `/tag` searches for `<tag>` only in the immediate children of the current node (i.e., from the root in a full path).\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Absolute path from the document root\n",
    "html.xpath('/html/body/div/p')\n",
    "\n",
    "# Relative path (searches anywhere in the document)\n",
    "html.xpath('//p')\n",
    "```\n",
    "\n",
    "## Attribute Conditions\n",
    "- `//div[@class=\"nav\"]` selects all `<div>` elements with `class=\"nav\"`.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Select all <img> elements whose \"alt\" attribute equals \"logo\"\n",
    "html.xpath('//img[@alt=\"logo\"]')\n",
    "```\n",
    "\n",
    "## Text Matching\n",
    "- `//td[text()=\"Genre\"]` matches a `<td>` whose entire text content is `\"Genre\"`.\n",
    "- `//td[contains(text(),\"Genre\")]` matches a `<td>` whose text content contains `\"Genre\"` as a substring.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Exact text match\n",
    "html.xpath('//span[text()=\"Subscribe\"]')\n",
    "\n",
    "# Partial text match (avoids issues with whitespace or additional text)\n",
    "html.xpath('//span[contains(text(), \"Subscribe\")]')\n",
    "```\n",
    "\n",
    "## Handling Whitespace & Newlines\n",
    "HTML often includes line breaks like `\\r\\n`. To handle partial matches, you can use:\n",
    "- `contains()`\n",
    "- Functions like `normalize-space()`\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Using contains() to avoid missing text with stray newline characters\n",
    "html.xpath('//td[contains(text(), \"Genre\")]')\n",
    "\n",
    "# Using normalize-space() if there's excessive spacing\n",
    "html.xpath('//td[normalize-space(text())=\"Genre\"]')\n",
    "```\n",
    "\n",
    "## Navigation Axes\n",
    "- `following-sibling::`, `preceding-sibling::`, `parent::`, `child::`, etc.  \n",
    "  These allow you to move in the document relative to a known node.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Select the text node following a <b> element with text 'Script Date'\n",
    "html.xpath('//b[text()=\"Script Date\"]/following-sibling::text()[1]')\n",
    "\n",
    "# Select any <div> that is the parent of an <img> with src=\"logo.png\"\n",
    "html.xpath('//img[@src=\"logo.png\"]/parent::div')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Additional XPath Grammar and Methods\n",
    "\n",
    "Below we introduce more XPath concepts, including wildcard usage, union operators, and common functions for more powerful queries.\n",
    "\n",
    "## Wildcards\n",
    "- `*` matches any element node (regardless of its name).\n",
    "- `@*` matches any attribute node.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Select all child elements under <div> of class \"container\", regardless of tag name\n",
    "html.xpath('//div[@class=\"container\"]/*')\n",
    "\n",
    "# Select all attributes of the <img> elements\n",
    "html.xpath('//img/@*')\n",
    "```\n",
    "\n",
    "## Union (|) Operator\n",
    "- Combines multiple XPath expressions so you can select multiple sets of nodes.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Select all <div> or <span> elements\n",
    "html.xpath('//div | //span')\n",
    "```\n",
    "\n",
    "## Common Functions\n",
    "- `starts-with(string, substring)`: Tests if `string` starts with `substring`.\n",
    "- `substring(string, start, length)`: Returns a portion of `string`.\n",
    "- `string-length(string)`: Returns the length of a string.\n",
    "- `count(node-set)`: Returns the number of nodes in a node set.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Select <a> elements whose href starts with \"https\"\n",
    "html.xpath('//a[starts-with(@href, \"https\")]')\n",
    "\n",
    "# Count how many <p> elements exist\n",
    "num_paragraphs = html.xpath('count(//p)')\n",
    "print(\"Number of <p> elements:\", num_paragraphs)\n",
    "```\n",
    "\n",
    "## Context Nodes and Parent/Child Notation\n",
    "- `.` refers to the current context node.\n",
    "- `..` refers to the parent of the current node.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# From a known element \"el\", select its parent's sibling divs\n",
    "el.xpath('../following-sibling::div')\n",
    "```\n",
    "\n",
    "## Putting It All Together\n",
    "When crafting your XPath, you often combine these features:\n",
    "1. Start with a known node or wildcard.\n",
    "2. Use predicate filters on attributes/text/position.\n",
    "3. Employ axes to move to siblings, parents, children, etc.\n",
    "4. Apply string functions or partial matches to handle real-world HTML quirks.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# 1. Find a table containing a <td> with text \"Genre\"\n",
    "# 2. Then locate all <a> within that table (in any row).\n",
    "genre_links = html.xpath('//table[tr/td[contains(text(), \"Genre\")]]//a/@href')\n",
    "\n",
    "# 3. Move from a known <b> element's text to the next text node.\n",
    "release_date_text = html.xpath('//b[text()=\"Script Date\"]/following-sibling::text()[1]')\n",
    "\n",
    "# 4. Use starts-with() to filter anchor links that begin with \"/scripts\".\n",
    "script_links = html.xpath('//a[starts-with(@href, \"/scripts\")]/@href')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "By understanding these XPath strategies and methods, you can become more agile in navigating and extracting data from HTML documents that vary in layout or contain dynamic elements. Always remember to inspect the **actual** HTML returned by your HTTP client (e.g., `requests`) rather than relying solely on the rendered DOM in a browser, which may include additional transformations or scripts.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a03a30",
   "metadata": {},
   "source": [
    "Consider the website [`https://imsdb.com/`](https://imsdb.com/). We want to scrape the links that are in the _Genre_ sidebar. Using devtools, we can inspect this element and find that its a child of `table/tbody`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e630b0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import lxml.html as lx\n",
    "result = requests.get('https://imsdb.com/')\n",
    "result.raise_for_status\n",
    "html = lx.fromstring(result.text)\n",
    "html.xpath('//table/tbody')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bb31a",
   "metadata": {},
   "source": [
    "The list is empty. Copying the xpath from devtools doesn't help either. Apparently, the html that the requests returns is not the same as the one rendered by Google Chrome. We can inspect whatever is being returned by checking the _Networks_ tab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38752d",
   "metadata": {},
   "source": [
    "While re-loading the webpage to monitor the communication in the _Networks_ tab, we note see that the html (for smaller dimensions which can be set in the upper left corner) is now rendered for mobile use. The sidebar with _Genre_ section is now missing. Going back to inspecting the html we see, that the genres are now listed as dropdown menu. The dropdown menu does not contain links, those are generated by a script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4530043",
   "metadata": {},
   "source": [
    "Back to the network tab! Cycling through all requests, we find that the html is returned as `Document`, but no other data is transferred. Lets inspect the request, and navigate to its _Response_ tab. We can search it for the string `Genres`. We find three instances, but all preparing the script, none containing the links. While dealing with scripts was presented in todays lecture, we should adjust the dimensions (upper left corner) to something larger (e.g., _Nest Hub Max_). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b89dca",
   "metadata": {},
   "source": [
    "A new request will now return a different html. Searching for the string `Genre` will now find the corresponding table, its in a different element structure as in our first attempt. However, ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath('//td[text()=\"Genre\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25e440",
   "metadata": {},
   "source": [
    "Some whitespace characters prevent us from finding the element! (Direct inspection of `request.text` shows that its `\"Genre\\r\\n\"`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath('//td[contains(text(), \"Genre\")]') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f41c7",
   "metadata": {},
   "source": [
    "Now, how to get the correct anchors? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c94f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "html.xpath('//table[tr/td[contains(text(), \"Genre\")]]/tr//a/@href') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc51903",
   "metadata": {},
   "source": [
    "Perfect! Now, consider the [_Interstellar_](https://imsdb.com/Movie%20Scripts/Interstellar%20Script.html) page. We want to retrieve the movie release year. After inspecting the html (it might not be accurate!), we find that the date is the content of a `<td>` element, but is cluttered between a variety of other elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get('https://imsdb.com/Movie%20Scripts/Interstellar%20Script.html')\n",
    "result.raise_for_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a44c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = lx.fromstring(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0bc896",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "html.xpath('//table[@class=\"script-details\"]//td/text()') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ca0cf",
   "metadata": {},
   "source": [
    "Its there, but how to we retrieve the correct element text? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a5bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath('//b[text() = \"Script Date\"]/following-sibling::text()[1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613f6b0",
   "metadata": {},
   "source": [
    "From here, we will use regular expressions to extract the digits of the year. We will learn about regular expressions next week. In the meantime, become an xpath [ninja](https://topswagcode.com/xpath/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ee2ac",
   "metadata": {},
   "source": [
    "# Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff3f282",
   "metadata": {},
   "source": [
    "## Tutorial: Beautiful Soup Basics\n",
    "\n",
    "\n",
    "```markdown\n",
    "# Tutorial: Beautiful Soup Basics\n",
    "\n",
    "Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping. Here's a brief step-by-step tutorial that outlines how to get started:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Installation\n",
    "\n",
    "Before using Beautiful Soup, you need to install it. If you haven’t already:\n",
    "\n",
    "```bash\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Importing and Creating a Soup Object\n",
    "\n",
    "To begin parsing HTML, import both `requests` (or another HTTP library) and `BeautifulSoup`:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch a webpage\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML text\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Alternatively, parse an HTML string directly\n",
    "html_doc = \"<html><body><p>Hello!</p></body></html>\"\n",
    "soup_from_string = BeautifulSoup(html_doc, \"html.parser\")\n",
    "```\n",
    "\n",
    "A `BeautifulSoup` object (`soup` in these examples) acts as a structured representation of your HTML. You can navigate and search it like a tree.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Parsing and Navigating the HTML Tree\n",
    "\n",
    "### 3.1 Accessing Elements by Tag\n",
    "\n",
    "```python\n",
    "# Access the first <title> tag found in the document\n",
    "page_title = soup.title\n",
    "\n",
    "# Access the first <body> tag\n",
    "page_body = soup.body\n",
    "\n",
    "# Access the first <p> tag\n",
    "paragraph = soup.p\n",
    "```\n",
    "\n",
    "Remember that these direct accesses (`soup.p`, `soup.title`) only give you **the first** occurrence of that tag.\n",
    "\n",
    "### 3.2 Going Down the Tree\n",
    "\n",
    "- `.contents` gives a list of **all children** of a tag.\n",
    "- `.children` is an **iterator** over those children.\n",
    "\n",
    "```python\n",
    "# If we want to see what's inside <body>\n",
    "print(soup.body.contents)\n",
    "\n",
    "# Or iterate over the children:\n",
    "for child in soup.body.children:\n",
    "    print(child)\n",
    "```\n",
    "\n",
    "### 3.3 Going Up the Tree\n",
    "\n",
    "If you have a tag, you can find its parent:\n",
    "\n",
    "```python\n",
    "# Access a tag's parent\n",
    "if soup.p:\n",
    "    parent_of_p = soup.p.parent\n",
    "    print(\"Parent of <p>:\", parent_of_p.name)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Finding Elements\n",
    "\n",
    "### 4.1 `find_all()`\n",
    "\n",
    "`find_all()` returns **all** matches of your query:\n",
    "\n",
    "```python\n",
    "# All paragraph tags\n",
    "paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "# All tags that have class=\"important\"\n",
    "important_tags = soup.find_all(class_=\"important\")\n",
    "```\n",
    "\n",
    "### 4.2 `find()`\n",
    "\n",
    "`find()` returns **the first** match:\n",
    "\n",
    "```python\n",
    "# First <p> tag with class \"important\"\n",
    "first_important_p = soup.find(\"p\", class_=\"important\")\n",
    "```\n",
    "\n",
    "### 4.3 CSS Selectors via `.select()`\n",
    "\n",
    "Use `.select()` to match elements using CSS selectors (like in a browser’s DevTools):\n",
    "\n",
    "```python\n",
    "# All <p> tags\n",
    "paragraphs_css = soup.select(\"p\")\n",
    "\n",
    "# A <p> tag with id=\"best-paragraph\"\n",
    "best_paragraph = soup.select(\"p#best-paragraph\")\n",
    "\n",
    "# A <p> tag with class=\"important\"\n",
    "important_paragraphs = soup.select(\"p.important\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Extracting Text and Attributes\n",
    "\n",
    "### 5.1 `.get_text()`\n",
    "\n",
    "`get_text()` returns **all** text within a tag (including its descendants), stripped of HTML tags:\n",
    "\n",
    "```python\n",
    "body_text = soup.body.get_text()\n",
    "print(body_text)\n",
    "```\n",
    "\n",
    "### 5.2 Accessing Attributes\n",
    "\n",
    "Tags can be treated like a dictionary to get/set attributes:\n",
    "\n",
    "```python\n",
    "some_link = soup.find(\"a\")\n",
    "if some_link:\n",
    "    href_value = some_link[\"href\"]   # Might raise KeyError if \"href\" missing\n",
    "    safer_href = some_link.get(\"href\", \"No link available\")\n",
    "    print(\"Link:\", safer_href)\n",
    "```\n",
    "\n",
    "You can also view the entire attributes dictionary:\n",
    "\n",
    "```python\n",
    "print(some_link.attrs)  # e.g., {\"href\": \"https://example.com\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Advanced Tasks\n",
    "\n",
    "Below are some advanced tasks you can perform with Beautiful Soup:\n",
    "\n",
    "1. **Filtering by Function**  \n",
    "   You can pass a function to `find_all()` or `find()` to define a custom matching condition.\n",
    "\n",
    "2. **Modifying the Parse Tree**  \n",
    "   You can insert, delete, or reorder tags within the parsed structure.\n",
    "\n",
    "3. **Handling Non-Standard Documents**  \n",
    "   Beautiful Soup is forgiving of poorly formed HTML, but you might need to experiment with different parsers (e.g., `\"html5lib\"`).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Output and Debugging\n",
    "\n",
    "### 7.1 `.prettify()`\n",
    "\n",
    "```python\n",
    "print(soup.prettify())      # Prints the entire HTML in a nicely formatted way\n",
    "print(soup.body.prettify()) # Prints just the <body> section\n",
    "```\n",
    "\n",
    "This helps you understand the structure that Beautiful Soup sees, which may differ from the raw HTML if there are minor errors.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Putting It All Together: Example\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Suppose there's a page containing multiple <div class=\"product\"> sections\n",
    "url = \"https://example.com/products\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 1: Locate all the product containers\n",
    "product_divs = soup.find_all(\"div\", class_=\"product\")\n",
    "\n",
    "# Step 2: Extract details from each product\n",
    "for product in product_divs:\n",
    "    title_tag = product.find(\"h2\")\n",
    "    price_tag = product.find(class_=\"price\")\n",
    "\n",
    "    if title_tag and price_tag:\n",
    "        product_name = title_tag.get_text().strip()\n",
    "        product_price = price_tag.get_text().strip()\n",
    "        print(f\"Product: {product_name} | Price: {product_price}\")\n",
    "```\n",
    "\n",
    "This small script:\n",
    "1. Downloads a webpage with `requests`.\n",
    "2. Parses it with `BeautifulSoup`.\n",
    "3. Finds multiple product `<div>` elements.\n",
    "4. Extracts their names and prices, printing them in a loop.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now have a quick overview of how Beautiful Soup “thinks” and how you can navigate, search, and extract data from HTML documents in Python. For more complex tasks—like dealing with JavaScript-heavy pages—consider using tools such as `Selenium` or `Playwright` to render dynamic content before scraping.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3b2bd",
   "metadata": {},
   "source": [
    "## Beautiful Soup   Example 1\n",
    "\n",
    "\n",
    "```markdown\n",
    "# Beautiful Soup\n",
    "\n",
    "Beautiful Soup is a library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for navigating, searching, and modifying the parse tree.\n",
    "\n",
    "Official documentation: [Beautiful Soup Docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Making the Soup\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# An example HTML snippet we want to parse:\n",
    "page = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>This is the Title!</title>\n",
    "</head>\n",
    "<body>\n",
    "    <p id=\"best-paragraph\">This is a paragraph!</p>\n",
    "    <p class=\"important\">This is another paragraph! &#127790;</p>\n",
    "    <p>Visit <a href=\"https://pudding.cool\">The Pudding</a>.</p>\n",
    "    <span class=\"important\">This is a span, it comes with a taco &#127790;</span>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Creating a BeautifulSoup object from the HTML string\n",
    "page_soup = BeautifulSoup(page, \"html.parser\")\n",
    "print(type(page_soup))  # -> <class 'bs4.BeautifulSoup'>\n",
    "```\n",
    "\n",
    "Beautiful Soup transforms the document into a tree of Python objects. Each element or piece of text becomes a node in this parse tree, accessible via the `page_soup` object.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Navigating the Tree\n",
    "\n",
    "### Navigating by Tag Type\n",
    "\n",
    "```python\n",
    "page_soup.head  # returns the <head> element\n",
    "page_soup.head.title  # returns the <title> element within <head>\n",
    "page_soup.p  # returns the first <p> element in the document\n",
    "```\n",
    "\n",
    "### Going Down: Children\n",
    "\n",
    "A tag’s children are the tags and strings nested within it.\n",
    "\n",
    "- **`.contents`** returns a **list** of a tag’s children.\n",
    "- **`.children`** returns an **iterator** for looping through the children.\n",
    "\n",
    "```python\n",
    "print(page_soup.body.contents)\n",
    "# This returns a list containing the <p> and <span> tags inside <body>.\n",
    "\n",
    "for child in page_soup.body.children:\n",
    "    print(child)\n",
    "# Iterates over each child element (including text nodes or whitespace) in <body>.\n",
    "```\n",
    "\n",
    "### Going Up: Parents\n",
    "\n",
    "You can access a tag’s parent with the **`.parent`** attribute.\n",
    "\n",
    "```python\n",
    "print(page_soup.title.parent)\n",
    "# Returns the parent of <title>, which is <head> in this case.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Searching the Tree\n",
    "\n",
    "Beautiful Soup provides methods to search for elements by tag, attributes, or custom filters.\n",
    "\n",
    "### `.find_all()`\n",
    "\n",
    "- **`.find_all()`** looks through a tag’s descendants and returns **all** matches.\n",
    "\n",
    "```python\n",
    "# Find all <p> tags\n",
    "all_paragraphs = page_soup.find_all(name=\"p\")\n",
    "print(all_paragraphs)\n",
    "\n",
    "# Find all tags with a specific id\n",
    "best_paragraph = page_soup.find_all(id=\"best-paragraph\")\n",
    "\n",
    "# Find all tags with a specific class\n",
    "important_tags = page_soup.find_all(class_=\"important\")  # class_ (underscore) is required for 'class'\n",
    "```\n",
    "\n",
    "You can also pass a dictionary for arbitrary attributes:\n",
    "\n",
    "```python\n",
    "page_soup.find_all(attrs={\"class\": \"important\"})\n",
    "```\n",
    "\n",
    "### `.find()`\n",
    "\n",
    "- **`.find()`** returns the **first** match in the document.\n",
    "\n",
    "```python\n",
    "first_title_tag = page_soup.find(name=\"title\")\n",
    "first_important_tag = page_soup.find(class_=\"important\")\n",
    "```\n",
    "\n",
    "### CSS Selector: `.select()`\n",
    "\n",
    "- **`.select()`** takes a **CSS selector** string and returns a list of all matching elements.\n",
    "\n",
    "```python\n",
    "page_soup.select(\"p\")                  # All <p> tags\n",
    "page_soup.select(\"p#best-paragraph\")   # <p> tag with id=\"best-paragraph\"\n",
    "page_soup.select(\"p.important\")        # <p> tag with class=\"important\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Contents and Attributes\n",
    "\n",
    "### `.get_text()`\n",
    "\n",
    "- **`.get_text()`** returns all the text beneath a tag (and its descendants) as a single string.\n",
    "\n",
    "```python\n",
    "print(page_soup.body.get_text())\n",
    "```\n",
    "\n",
    "### Accessing Attributes\n",
    "\n",
    "Tags in Beautiful Soup can be treated like dictionaries to access attributes.\n",
    "\n",
    "```python\n",
    "example_p = page_soup.p\n",
    "print(example_p[\"id\"])       # If <p> has an id attribute\n",
    "print(example_p.get(\"id\"))   # Safer access, returns None if not present\n",
    "```\n",
    "\n",
    "You can view all attributes of a tag using **`.attrs`**:\n",
    "\n",
    "```python\n",
    "print(example_p.attrs)  # A dictionary of all attributes on the <p> tag\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Output\n",
    "\n",
    "### `.prettify()`\n",
    "\n",
    "- **`.prettify()`** will return a nicely formatted (indented) string representation of the soup or a tag.\n",
    "\n",
    "```python\n",
    "print(page_soup.prettify())      # Pretty-print the entire document\n",
    "print(page_soup.body.prettify()) # Pretty-print just the <body> tag\n",
    "```\n",
    "\n",
    "This is useful for inspecting the structure of the parsed HTML in a more human-readable form.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Summary\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Suppose we retrieve a page with requests (instead of a raw string).\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# 1. Find elements by tag\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# 2. Use CSS selectors\n",
    "div_items = soup.select(\"div.item\")\n",
    "\n",
    "# 3. Extract text\n",
    "for item in div_items:\n",
    "    print(item.get_text())\n",
    "\n",
    "# 4. Access attributes\n",
    "for link in links:\n",
    "    href = link.get(\"href\")\n",
    "    print(\"Link:\", href)\n",
    "```\n",
    "\n",
    "By using Beautiful Soup’s intuitive methods, you can parse HTML documents, navigate their structure, and extract exactly the information you need.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a30837",
   "metadata": {},
   "source": [
    "# Example 2: National Weather Service\n",
    "\n",
    "Let's scrape the [National Weather Service](https://weather.gov/) for the weather forecast of Davis, CA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b4cfe",
   "metadata": {},
   "source": [
    "## Annotated Example: Scraping the National Weather Service\n",
    "\n",
    "\n",
    "```markdown\n",
    "# Scraping the National Weather Service: Davis, CA Forecast\n",
    "\n",
    "In this example, we’ll demonstrate how to:\n",
    "1. Send an HTTP request to the National Weather Service page for Davis, CA.\n",
    "2. Parse the HTML response with Beautiful Soup.\n",
    "3. Extract specific weather data (period names, short descriptions, temperatures, and detailed descriptions).\n",
    "4. Assemble the data into a pandas DataFrame.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Identify the Target URL\n",
    "# ----------------------------------------------------------------------------\n",
    "# This URL points to the 7-day forecast for Davis, CA on the National Weather Service website.\n",
    "url = \"https://forecast.weather.gov/MapClick.php?lat=38.54669000000007&lon=-121.74456999999995#.Y9fY5vv565t\"\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 2: Fetch the Page Content\n",
    "# ----------------------------------------------------------------------------\n",
    "# We use the requests library to get the page. \n",
    "# Calling 'raise_for_status()' ensures we raise an exception for any HTTP errors (e.g., 404, 500).\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 3: Parse HTML with BeautifulSoup\n",
    "# ----------------------------------------------------------------------------\n",
    "# Parse the retrieved HTML text, building a Soup object for further navigation and search.\n",
    "html_soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 4: Identify and Extract the \"Seven-Day Forecast\" Section\n",
    "# ----------------------------------------------------------------------------\n",
    "# The page contains a <div> tag with id=\"seven-day-forecast-container\", \n",
    "# which holds the daily weather forecast data.\n",
    "seven_day = html_soup.find(id=\"seven-day-forecast-container\")\n",
    "print(seven_day.prettify())  # This pretty-prints the found HTML (useful for debugging)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 5: Extract the Forecast Period Names\n",
    "# ----------------------------------------------------------------------------\n",
    "# We look for <p> tags with class=\"period-name\". \n",
    "# Each period typically contains a label like \"Tonight\", \"Monday\", \"Monday Night\", etc.\n",
    "period_names = seven_day.find_all(\"p\", class_=\"period-name\")\n",
    "# We then gather the text content of each matching tag into a list.\n",
    "period = [name.get_text() for name in period_names]\n",
    "print(period)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 6: Extract the Short Weather Descriptions\n",
    "# ----------------------------------------------------------------------------\n",
    "# The short weather descriptions are contained in <p> tags with class=\"short-desc\".\n",
    "descs = seven_day.find_all(\"p\", {\"class\": \"short-desc\"})\n",
    "description = [desc.get_text() for desc in descs]\n",
    "print(description)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 7: Extract the Temperatures\n",
    "# ----------------------------------------------------------------------------\n",
    "# Temperatures often have either \"temp-hi\" or \"temp-lo\" in their class name. \n",
    "# We use a CSS selector that matches any <p> tag whose class attribute *contains* the string \"temp\".\n",
    "temps = seven_day.select(\"p[class*='temp']\")\n",
    "temperature = [temp.get_text() for temp in temps]\n",
    "print(temperature)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 8: Extract Detailed Descriptions\n",
    "# ----------------------------------------------------------------------------\n",
    "# The images within each tombstone-container have a \"title\" attribute \n",
    "# that provides an extended weather description. \n",
    "# We use a CSS selector to find <img> elements inside <div class=\"tombstone-container\">.\n",
    "images = seven_day.select(\"div.tombstone-container img\")\n",
    "\n",
    "# \"attrs\" is a dictionary of HTML attributes. We extract the \"title\" value from each <img>.\n",
    "details = [image.attrs[\"title\"] for image in images]\n",
    "print(details)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 9: Clean Up the Detailed Descriptions\n",
    "# ----------------------------------------------------------------------------\n",
    "# The text might look like \"Tonight: Clear. Low around 39. Northwest wind 7 to 9 mph.\"\n",
    "# Notice how the portion before the colon might be a time period label like \"Tonight\". \n",
    "# We only want the forecast description part.\n",
    "example_detail = details[1]\n",
    "print(example_detail)\n",
    "\n",
    "# The .partition(\":\") method splits the string into three parts:\n",
    "#   1) the substring before the colon\n",
    "#   2) the colon itself\n",
    "#   3) the substring after the colon\n",
    "# Here, we only want the part after the colon (index [2]) and then .strip() to remove extra whitespace.\n",
    "cleaned_detail = example_detail.partition(\":\")[2].strip()\n",
    "print(cleaned_detail)\n",
    "\n",
    "# We can apply this to every item in the list to remove the period/time label:\n",
    "new_details = [detail.partition(\":\")[2].strip() for detail in details]\n",
    "print(new_details)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 10: Assemble the Data into a Pandas DataFrame\n",
    "# ----------------------------------------------------------------------------\n",
    "# We create a dictionary that maps column names to the lists we collected,\n",
    "# then pass that dictionary to pd.DataFrame.\n",
    "weather = pd.DataFrame({\n",
    "    \"Period\": period,\n",
    "    \"Description\": description,\n",
    "    \"Temperature\": temperature,\n",
    "    \"Detail\": new_details\n",
    "})\n",
    "\n",
    "print(weather)\n",
    "```\n",
    "\n",
    "## Explanation\n",
    "\n",
    "1. **Get the webpage**:  \n",
    "   We use the `requests` library to fetch the page’s HTML. The URL points to the specific latitude/longitude location for Davis, CA on the National Weather Service site.\n",
    "\n",
    "2. **Parse with Beautiful Soup**:  \n",
    "   `BeautifulSoup(response.text, \"html.parser\")` takes the raw HTML and converts it into a soup object, allowing us to search and navigate the HTML.\n",
    "\n",
    "3. **Find the “seven-day-forecast-container”**:  \n",
    "   This `<div>` element groups the day-by-day forecast information.  \n",
    "\n",
    "4. **Extracting data**:  \n",
    "   - **Period**: Inside each day’s weather block, the name of the period (like “Today,” “Tonight,” or “Tuesday”) appears in a `<p>` with class “period-name.”  \n",
    "   - **Short Description**: Found in `<p class=\"short-desc\">`.  \n",
    "   - **Temperature**: These appear in `<p>` elements whose class contains “temp” (e.g., “temp hi” or “temp lo”).  \n",
    "   - **Detailed Description**: Found within the `title` attribute of `<img>` elements. That text often includes the period name plus forecast details, so we split the string at the colon (`:`) to separate them.\n",
    "\n",
    "5. **DataFrame Construction**:  \n",
    "   We gather the extracted data in lists and convert them to a tidy DataFrame. The resulting table has columns for the period, short weather description, temperature, and a fully detailed forecast description.\n",
    "\n",
    "This approach shows how to use **Beautiful Soup** effectively for structured web scraping tasks. For dynamic pages (relying heavily on JavaScript), additional tools like Selenium or Playwright might be needed. However, many sites (like this NWS page) provide static HTML that you can scrape directly.\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
