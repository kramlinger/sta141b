{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a04a234a",
   "metadata": {},
   "source": [
    "### Week 7 - Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5cdd66",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# NLP Tutorial with Definitions\n",
    "\n",
    "Below is **one** raw Markdown chunk containing explanatory text, **key definitions**, and multiple Python code snippets.  \n",
    "All code fences use triple backticks with language spec, and this entire content is wrapped in an extra set of backticks to remain raw.  \n",
    "No code execution output is included.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "Natural Language Processing (NLP) is the field of enabling computers to understand, interpret, and generate human language. Python libraries such as **NLTK**, **TextBlob**, **spaCy**, and **gensim** help accomplish various NLP tasks including tokenization, parsing, and semantic analysis.\n",
    "\n",
    "This tutorial shows how to use **NLTK** and **regular expressions** (`re`) for the basic task of text **tokenization**. We also define key terms for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Key Definitions\n",
    "\n",
    "1. **Natural Language Processing (NLP)**  \n",
    "   Using computers to analyze, understand, or generate human language (e.g., English, Spanish). NLP often deals with tasks like sentiment analysis, language translation, topic modeling, etc.\n",
    "\n",
    "2. **Corpus**  \n",
    "   A **corpus** is a large and structured set of texts (or documents). Examples include a database of tweets, a collection of news articles, or the entire text of several books.\n",
    "\n",
    "3. **Document**  \n",
    "   A **document** is usually the smallest standalone text unit within a corpus. For example, one news article, one tweet, or one book in a set of books.\n",
    "\n",
    "4. **Token**  \n",
    "   A **token** is a sequence of characters treated as a single meaningful entity. Typically, tokens are words, but they can also be punctuation marks, hashtags, or other atomic units.\n",
    "\n",
    "5. **Tokenization**  \n",
    "   The process of splitting text into smaller pieces called *tokens*. It is often the first step in text preprocessing.\n",
    "\n",
    "6. **Regular Expression (Regex)**  \n",
    "   A **regular expression** is a special text string used for describing search patterns. In Python, the `re` module provides functions to match or split strings using regexes.\n",
    "\n",
    "7. **Stopwords**  \n",
    "   **Stopwords** are common words (e.g., “and,” “the,” “to”) that usually carry little meaning in text analysis. Removing them can often improve analysis results.\n",
    "\n",
    "8. **Stemming**  \n",
    "   **Stemming** crudely chops off word endings to reduce words to their base forms (e.g., “running” → “run”).\n",
    "\n",
    "9. **Lemmatization**  \n",
    "   **Lemmatization** also reduces words to a base form (lemma), but it uses vocabulary and morphological analysis (e.g., “am,” “are,” “is” → “be”).\n",
    "\n",
    "10. **Frequency Distribution**  \n",
    "    A summary of how often different tokens (words) appear in a text. Helps find the most frequent words.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Installing and Loading NLTK\n",
    "\n",
    "1. **Install NLTK** (if needed):\n",
    "   ```bash\n",
    "   pip install nltk\n",
    "   ```\n",
    "2. **Import NLTK and Download Resources**:\n",
    "   ```python\n",
    "   import nltk\n",
    "   nltk.download('punkt')      # for tokenizers\n",
    "   nltk.download('gutenberg')  # sample Project Gutenberg texts\n",
    "   nltk.download('stopwords')  # for stopword lists\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Using NLTK for Tokenization\n",
    "\n",
    "### 4.1 Loading a Corpus (Gutenberg Example)\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# List file IDs in the Gutenberg corpus\n",
    "print(gutenberg.fileids())\n",
    "\n",
    "# Load the raw text of \"Moby Dick\"\n",
    "moby_raw = gutenberg.raw(\"melville-moby_dick.txt\")\n",
    "print(moby_raw[:500])  # print the first 500 characters\n",
    "```\n",
    "\n",
    "### 4.2 Sentence Tokenization\n",
    "\n",
    "```python\n",
    "# Split text into sentences\n",
    "sentences = nltk.sent_tokenize(moby_raw)\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(sentences[0])  # view the first sentence\n",
    "```\n",
    "\n",
    "### 4.3 Word Tokenization\n",
    "\n",
    "```python\n",
    "# Split the raw text into individual word tokens\n",
    "words = nltk.word_tokenize(moby_raw)\n",
    "print(f\"Number of word tokens: {len(words)}\")\n",
    "print(words[:20])  # see the first 20 tokens\n",
    "```\n",
    "\n",
    "### 4.4 Tokenizing a Custom Sentence\n",
    "\n",
    "```python\n",
    "custom_sentence = \"Hello, world! This is a test-sentence, with punctuation.\"\n",
    "custom_tokens = nltk.word_tokenize(custom_sentence)\n",
    "print(custom_tokens)\n",
    "# Example output (not shown here to keep it raw):\n",
    "# ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test-sentence', ',', 'with', 'punctuation', '.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Splitting Text with Regular Expressions\n",
    "\n",
    "### 5.1 Splitting on Non-Word Characters\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "sample_text = \"Hello, world!\\nThis is regex 101.\\tLet's split by non-word chars.\"\n",
    "split_tokens = re.split(r\"\\W+\", sample_text)\n",
    "print(split_tokens)\n",
    "# Might include empty strings where the pattern matches at start/end\n",
    "```\n",
    "\n",
    "### 5.2 Finding Words with `re.findall`\n",
    "\n",
    "```python\n",
    "# Grab all word-like sequences (\\w+)\n",
    "words_regex = re.findall(r\"\\w+\", sample_text)\n",
    "print(words_regex)\n",
    "# Potentially: ['Hello', 'world', 'This', 'is', 'regex', '101', 'Let', 's', 'split', 'by', 'non', 'word', 'chars']\n",
    "```\n",
    "\n",
    "### 5.3 Extracting Chapter Headings from Moby Dick\n",
    "\n",
    "```python\n",
    "moby_chapters = re.findall(r\"(CHAPTER\\s+\\d+.*?)\\r?\\n\\r?\\n\", moby_raw)\n",
    "print(f\"Found {len(moby_chapters)} potential chapters.\")\n",
    "# Each element in moby_chapters should start with “CHAPTER <number>” up to a blank line.\n",
    "```\n",
    "\n",
    "### 5.4 Extracting Specific Chapter Text\n",
    "\n",
    "```python\n",
    "# Grab \"CHAPTER 1\" text up until the next \"CHAPTER <number>\"\n",
    "chapter_1_pattern = r\"(CHAPTER 1.*?)(?=CHAPTER \\d+|$)\"\n",
    "match = re.search(chapter_1_pattern, moby_raw, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    chapter_1_text = match.group(1)\n",
    "    chapter_1_tokens = nltk.word_tokenize(chapter_1_text)\n",
    "    print(f\"Number of tokens in Chapter 1: {len(chapter_1_tokens)}\")\n",
    "else:\n",
    "    print(\"Chapter 1 not found.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Further NLP Examples\n",
    "\n",
    "### 6.1 Frequency Distribution\n",
    "\n",
    "```python\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Convert to lowercase and create a frequency distribution\n",
    "words_lower = [w.lower() for w in words]\n",
    "fdist = FreqDist(words_lower)\n",
    "\n",
    "# Show the top 10 most common tokens\n",
    "common_tokens = fdist.most_common(10)\n",
    "print(common_tokens)\n",
    "```\n",
    "\n",
    "### 6.2 Removing Stopwords\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [w for w in words_lower if w.isalpha() and w not in stop_words]\n",
    "\n",
    "print(f\"Original token count: {len(words_lower)}\")\n",
    "print(f\"Filtered token count: {len(filtered_tokens)}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "- **Definition Recap**:\n",
    "  - **NLP**: Processing human language with computers.\n",
    "  - **Corpus/Document**: Structured text collections vs. individual text units.\n",
    "  - **Token/Tokenization**: Splitting text into meaningful chunks.\n",
    "  - **Regex**: Pattern-based string manipulation.\n",
    "  - **Stopwords**, **Stemming**, **Lemmatization**: Techniques to clean and normalize textual data.\n",
    "- **NLTK** provides:\n",
    "  - Ready-to-use corpora (e.g., Gutenberg).\n",
    "  - Easy tokenization methods (`sent_tokenize`, `word_tokenize`).\n",
    "  - Tools for frequency analysis, stopword removal, etc.\n",
    "- **Regular Expressions** (`re`) let you handle custom or advanced pattern matching.\n",
    "\n",
    "From here, you can move into more advanced tasks like **Part-of-Speech Tagging**, **Named Entity Recognition**, **Text Classification**, or explore other libraries like **spaCy** for fast production-level NLP.\n",
    "\n",
    "``` \n",
    "````````markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13073ce",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Regular Expressions: Basics, Grammar, and Homework-Specific Usage\n",
    "\n",
    "This tutorial explains **regular expressions (regex)** from the ground up for students with little or no prior experience, highlighting patterns, methods, and examples needed for the Wiki Game and Movie Scraping homework in STA 141B.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Regex Basics\n",
    "\n",
    "Regular expressions are a mini-language for **pattern matching** and **searching** within text. In Python, they are provided by the built-in [`re` module](https://docs.python.org/3/library/re.html).\n",
    "\n",
    "### 1.1. Metacharacters and Special Symbols\n",
    "\n",
    "- **`.` (dot)**: Matches **any single character** (except possibly newlines, unless you set certain flags like `re.DOTALL`).\n",
    "- **`^`**: Asserts the **start of a string** (or start of a line with `re.MULTILINE`).\n",
    "- **`$`**: Asserts the **end of a string** (or end of a line with `re.MULTILINE`).\n",
    "- **`*`**: Matches **0 or more** of the preceding element.\n",
    "- **`+`**: Matches **1 or more** of the preceding element.\n",
    "- **`?`**: \n",
    "  1. Matches **0 or 1** of the preceding element (makes something optional), or \n",
    "  2. Makes a quantifier (like `*` or `+`) **non-greedy** when combined (`*?`, `+?`).\n",
    "- **`{m,n}`**: Matches **m to n** of the preceding element, e.g., `\\d{2,4}` matches 2–4 digits in a row.\n",
    "- **`(...)`**: **Parentheses** for **grouping** or **capturing**. \n",
    "- **`|`**: **Alternation**, acts like a logical OR. For example, `(cat|dog)` matches `\"cat\"` or `\"dog\"`.\n",
    "- **`\\`**: Escape character. For instance, `\\.` matches a literal dot (`.`), `\\(` matches a literal `(`, etc.\n",
    "\n",
    "Below are short code examples demonstrating these metacharacters:\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text_sample = \"cat cot cut c#t c t\"\n",
    "\n",
    "# 1) Dot (.)\n",
    "#    Pattern: \"c.t\" matches any single character between 'c' and 't'\n",
    "match_dot = re.findall(r\"c.t\", text_sample)\n",
    "print(\"Matches for c.t:\", match_dot)\n",
    "# Example output might be ['cat', 'cot', 'cut']\n",
    "\n",
    "# 2) Start (^) and End ($)\n",
    "lines = \"\"\"dog\n",
    "cat\n",
    "car\n",
    "cart\n",
    "\"\"\"\n",
    "# Matches lines that start with 'c' and end with 't'\n",
    "match_start_end = re.findall(r\"^c.*t$\", lines, flags=re.MULTILINE)\n",
    "print(\"Lines starting with c and ending with t:\", match_start_end)\n",
    "# Example output: ['cat', 'cart']\n",
    "\n",
    "# 3) Star (*) vs Plus (+) vs Question Mark (?)\n",
    "text_nums = \"ab, abb, abbb, abbbb\"\n",
    "# 'ab*' => \"a\" followed by zero or more \"b\"\n",
    "match_star = re.findall(r\"ab*\", text_nums)\n",
    "print(\"Matches for ab*:\", match_star)\n",
    "# Might match 'a', 'ab', 'abb', 'abbb', etc. (depending on context)\n",
    "\n",
    "# 'ab+' => \"a\" followed by one or more \"b\"\n",
    "match_plus = re.findall(r\"ab+\", text_nums)\n",
    "print(\"Matches for ab+:\", match_plus)\n",
    "\n",
    "# 'ab?' => \"a\" followed by zero or one \"b\"\n",
    "match_question = re.findall(r\"ab?\", text_nums)\n",
    "print(\"Matches for ab?:\", match_question)\n",
    "\n",
    "# 4) Curly Braces {m,n}\n",
    "text_digits = \"Phone: 1234567, Code: 999, Number: 1234\"\n",
    "# Pattern for 3 to 4 digits in a row\n",
    "match_digits = re.findall(r\"\\b\\d{3,4}\\b\", text_digits)\n",
    "print(\"Matches for 3-4 digits:\", match_digits)\n",
    "\n",
    "# 5) Grouping and Alternation\n",
    "animals = \"cat dog mouse rat catdog\"\n",
    "# Pattern: (cat|dog)\n",
    "group_alt = re.findall(r\"(cat|dog)\", animals)\n",
    "print(\"Matches for (cat|dog):\", group_alt)\n",
    "# Will match 'cat', 'dog', but not 'catdog' in a single match\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2. Character Classes\n",
    "\n",
    "- **`[ABC]`**: Matches **one** character that is `A`, `B`, or `C`.\n",
    "- **`[A-Za-z0-9]`**: Matches any alphanumeric character.\n",
    "- **`[^...]`**: **Negated** character class. For instance, `[^0-9]` matches any character *not* a digit.\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text_chars = \"ABC 123 #$% xyz\"\n",
    "\n",
    "# [ABC]\n",
    "match_abc = re.findall(r\"[ABC]\", text_chars)\n",
    "print(\"Matches for [ABC]:\", match_abc)\n",
    "\n",
    "# Negated class [^0-9] => non-digit\n",
    "match_non_digit = re.findall(r\"[^0-9]+\", text_chars)\n",
    "print(\"Matches for [^0-9]+:\", match_non_digit)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3. Common Shorthand Escapes\n",
    "\n",
    "- **`\\d`**: Digit (0–9).\n",
    "- **`\\s`**: Whitespace (spaces, tabs, newlines, etc.).\n",
    "- **`\\w`**: Word characters (letters, digits, underscores).\n",
    "- **`\\b`**: Word boundary.\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "sample_text = \"ABC123 foo_bar   7\"\n",
    "\n",
    "# 1) \\d => digit\n",
    "digits = re.findall(r\"\\d\", sample_text)\n",
    "print(\"Digits:\", digits)\n",
    "\n",
    "# 2) \\s => whitespace\n",
    "spaces = re.findall(r\"\\s+\", sample_text)\n",
    "print(\"Whitespace chunks:\", spaces)\n",
    "\n",
    "# 3) \\w => word characters (letters, digits, underscore)\n",
    "words = re.findall(r\"\\w+\", sample_text)\n",
    "print(\"Word chunks:\", words)\n",
    "\n",
    "# 4) \\b => word boundary\n",
    "boundary_test = re.findall(r\"\\bfoo\\b\", \"foo_bar foo bar foobar\")\n",
    "print(\"Exact 'foo' word:\", boundary_test)\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The `re` Module: Common Methods\n",
    "\n",
    "1. **`re.search(pattern, string)`**: Finds the *first* occurrence of `pattern`.  \n",
    "2. **`re.match(pattern, string)`**: Like `search` but only at the beginning of the string.  \n",
    "3. **`re.findall(pattern, string)`**: Returns a list of all **non-overlapping** matches.  \n",
    "4. **`re.finditer(pattern, string)`**: Returns an iterator of match objects.  \n",
    "5. **`re.sub(pattern, repl, string)`**: **Substitutes** matches of `pattern` with `repl`.  \n",
    "6. **`re.split(pattern, string)`**: Splits `string` using `pattern` as a delimiter.  \n",
    "7. **`re.compile(pattern)`**: Compiles a pattern for repeated use.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text = \"Hello world! 2023 is a year, 1999 was another year.\"\n",
    "\n",
    "# 1) re.search\n",
    "match_search = re.search(r\"\\d{4}\", text)\n",
    "print(match_search.group())  # '2023' (the first 4-digit number found)\n",
    "\n",
    "# 2) re.findall\n",
    "all_numbers = re.findall(r\"\\d{4}\", text)\n",
    "print(all_numbers)  # ['2023', '1999']\n",
    "\n",
    "# 3) re.sub\n",
    "text_no_digits = re.sub(r\"\\d+\", \"[NUM]\", text)\n",
    "print(text_no_digits)\n",
    "# \"Hello world! [NUM] is a year, [NUM] was another year.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why Parentheses in Regex?\n",
    "\n",
    "1. **Grouping**: `(19|20)\\d{2}` means “19 or 20, followed by 2 digits.” Without parentheses, `19|20\\d{2}` might be interpreted as “19 **or** `20\\d{2}`,” which could break your intended logic.  \n",
    "2. **Capturing**: If you do `match = re.search(r\"(19|20)\\d{2}\", text)`, you can use `match.group(1)` to see which part of the group (`19` or `20`) was matched.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Homework-Specific Patterns & Logic\n",
    "\n",
    "### 4.1. Removing Parenthetical Text\n",
    "\n",
    "**Goal**: If a string contains `( ... )`, you often want to remove them entirely (e.g., in Wiki articles).\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text_example = \"Some text (hidden info) more text (another) done.\"\n",
    "removed = re.sub(r\"\\(.*?\\)\", \"\", text_example)\n",
    "print(removed)\n",
    "# Output: \"Some text  more text  done.\"\n",
    "```\n",
    "\n",
    "### 4.2. Ignoring External or Special Links (e.g., `/wiki/File:`)\n",
    "\n",
    "**Goal**: In the Wiki Game, skip `/wiki/File:`, `/wiki/Category:`, etc.\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "def is_valid_link(url):\n",
    "    # Return False if it matches these special forms\n",
    "    if re.match(r\"^/wiki/(File|Category|Special):\", url):\n",
    "        return False\n",
    "    # Skip external 'http' or disambiguation pages\n",
    "    if re.match(r\"^http\", url) or \"disambiguation\" in url.lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "print(is_valid_link(\"/wiki/File:Example.jpg\"))  # False\n",
    "print(is_valid_link(\"/wiki/Regular_Article\"))   # True\n",
    "```\n",
    "\n",
    "### 4.3. Removing Italicized Sections\n",
    "\n",
    "To ignore italicized links (`<i><a href=...>`), remove all `<i>...</i>` blocks:\n",
    "\n",
    "```python\n",
    "html = \"<p>Text <i><a href='/wiki/Italics'>Ignore me</a></i> and <a href='/wiki/Valid'>Use me</a>.</p>\"\n",
    "cleaned = re.sub(r\"<i>.*?</i>\", \"\", html, flags=re.DOTALL)\n",
    "print(cleaned)\n",
    "# \"<p>Text  and <a href='/wiki/Valid'>Use me</a>.</p>\"\n",
    "```\n",
    "\n",
    "### 4.4. Matching Years (e.g., 1900–2099)\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text_movie = \"Script date: 2004. Release date: 2006. Old year: 1879.\"\n",
    "years = re.findall(r\"\\b\\d{4}\\b\", text_movie)\n",
    "print(years)  # ['2004', '2006', '1879']\n",
    "\n",
    "modern_years = re.findall(r\"(?:19|20)\\d{2}\", text_movie)\n",
    "print(modern_years)\n",
    "# ['2004', '2006'] (1879 won't match)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Putting It All Together: Wiki Game Example\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "page_html = \"\"\"\n",
    "<p>This is an example (ignore me) <i><a href=\"/wiki/Italics\">Italics link</a></i>\n",
    "<a href=\"/wiki/File:Example.jpg\">File link</a>\n",
    "<a href=\"/wiki/Valid_Link\">Go here</a></p>\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Remove parentheses\n",
    "step1 = re.sub(r\"\\(.*?\\)\", \"\", page_html)\n",
    "\n",
    "# Step 2: Remove italic tags\n",
    "step2 = re.sub(r\"<i>.*?</i>\", \"\", step1, flags=re.DOTALL)\n",
    "\n",
    "# Step 3: Extract links with a regex for href=\"...\"\n",
    "links = re.findall(r'href=\"([^\"]+)\"', step2)\n",
    "\n",
    "# Step 4: Filter out invalid links (File, Category, Special, http)\n",
    "valid_links = []\n",
    "for lk in links:\n",
    "    if re.match(r\"^/wiki/(File|Category|Special):\", lk):\n",
    "        continue\n",
    "    if lk.startswith(\"http\"):\n",
    "        continue\n",
    "    valid_links.append(lk)\n",
    "\n",
    "print(\"Valid links found:\", valid_links)\n",
    "# Next: take the first valid link, follow it, repeat the process in your Wiki Game.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "With these **regex basics** and **examples** in hand, you can:\n",
    "1. **Skip** or **remove** unwanted text (parentheses, italic tags, special links).\n",
    "2. **Extract** relevant data (years, normal wiki links).\n",
    "3. **Build** the Wiki Game logic (follow valid links, handle loops).\n",
    "4. **Scrape** or parse text from HTML by focusing on stable patterns (like `<a href=\"...\">`) and ignoring sections you don't need.\n",
    "\n",
    "**Key Takeaways**:\n",
    "- **Master** the core symbols (`.`, `^`, `$`, `*`, `+`, `?`, `{m,n}`, `(...)`, `|`) and how to **escape** them (`\\(`, `\\.`).\n",
    "- **Use** `re.findall`, `re.search`, `re.sub` wisely.\n",
    "- **Test** your regex thoroughly on example strings to ensure correctness for edge cases.\n",
    "\n",
    "Happy coding and good luck with your homework!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e022a0",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# NLTK Grammar and Methods for STA 141B Homework\n",
    "\n",
    "In some STA 141B tasks, you may want to go beyond simple **regex** or **HTML parsing** and leverage more robust **Natural Language Processing (NLP)** approaches. The **Natural Language Toolkit (NLTK)** is a popular Python library that provides tools for tokenizing text, tagging parts of speech, parsing syntax, and more. Below is a brief overview of NLTK basics that might be helpful if your homework requires deeper text analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Installation & Basic Setup\n",
    "\n",
    "If you are working in a standard environment (e.g., Anaconda, pip), install NLTK with:\n",
    "\n",
    "```bash\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "Then, within Python, you can download certain corpora or tokenizers:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')  # For tokenizers\n",
    "nltk.download('averaged_perceptron_tagger')  # For part-of-speech tagging\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Tokenizing Text\n",
    "\n",
    "- **Sentence Tokenization**: Splits a large text into individual sentences.  \n",
    "- **Word Tokenization**: Splits a sentence into words (tokens).\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "\n",
    "text = \"Hello world! This is a test. Let's see how NLTK tokenizes sentences.\"\n",
    "\n",
    "# Sentence Tokenize\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(\"Sentences:\", sentences)\n",
    "\n",
    "# Word Tokenize each sentence\n",
    "for i, sent in enumerate(sentences, start=1):\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    print(f\"Sentence {i} tokens:\", tokens)\n",
    "```\n",
    "\n",
    "You could use this approach, for example, if you want to detect and skip parentheses or italic sections at a token level rather than relying solely on regex.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Part-of-Speech (POS) Tagging\n",
    "\n",
    "If you need to determine whether a token is a noun, verb, adjective, etc., you can use **POS tagging**. This might be helpful if you need to filter certain words or analyze link anchor text based on grammar.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "\n",
    "sentence = \"Wiki articles often contain many references.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)\n",
    "# Example output might be: [('Wiki', 'NNP'), ('articles', 'NNS'), ('often', 'RB'), ('contain', 'VBP'), ...]\n",
    "```\n",
    "\n",
    "Each tuple has the form `(word, POS_tag)`. For instance, `NN` = noun (singular), `NNS` = noun (plural), `VBP` = verb present tense, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Chunking & Parsing\n",
    "\n",
    "For more advanced tasks, you can define **chunk grammars** to group tokens into phrases:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"  # A simple grammar for a Noun Phrase: optional Det, any # of Adjs, then a Noun\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "text_sample = \"The big dog jumped over the lazy fox.\"\n",
    "tokens = nltk.word_tokenize(text_sample)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "\n",
    "print(tree)\n",
    "tree.draw()  # If you have a GUI environment, this displays a tree diagram\n",
    "```\n",
    "\n",
    "**Relevance to Homework**: Usually, chunking might not be strictly required unless you are analyzing the grammatical structure of anchor texts or script lines. But it’s good to know that NLTK can handle these tasks if your assignment asks for more nuanced textual patterns beyond regex.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Potential Use-Cases in Homework\n",
    "\n",
    "1. **Filtering Certain Terms**: After tokenizing, you could remove stop words or punctuation.  \n",
    "2. **Detecting Key Phrases**: If the assignment requires analyzing script lines or complex textual fields, NLTK can parse or chunk them.  \n",
    "3. **Advanced Searching**: Instead of relying purely on `re.search`, you can tokenize, then do logical checks (e.g., skip tokens in parentheses or ignore italicized segments).\n",
    "\n",
    "> **Note**: In many STA 141B use-cases, **regex + HTML parsing** will suffice. NLTK is an extra layer for more complex text manipulation (e.g., ignoring certain words by part of speech, or extracting lines that contain specific grammatical structures).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Practical Example\n",
    "\n",
    "Below is a simplified example showing how you might combine **regex** to remove HTML tags, then **NLTK** to tokenize or further analyze the clean text:\n",
    "\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Suppose you scraped some raw HTML:\n",
    "raw_html = \"<p>This is <i>italic text</i> in a sample script (1985 draft version).</p>\"\n",
    "\n",
    "# 1) Remove HTML tags with regex\n",
    "text_no_html = re.sub(r\"<.*?>\", \"\", raw_html)\n",
    "\n",
    "# 2) Remove parentheses text with regex\n",
    "text_clean = re.sub(r\"\\(.*?\\)\", \"\", text_no_html)\n",
    "\n",
    "# 3) Tokenize with NLTK\n",
    "tokens = nltk.word_tokenize(text_clean)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 4) Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "```\n",
    "\n",
    "This approach is helpful if your assignment requires **linguistic** or **grammatical** analysis after stripping out extraneous HTML elements.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Summary\n",
    "\n",
    "- **NLTK** provides a higher-level way to work with textual data, beyond simple regex.  \n",
    "- **Tokenizing** text into sentences or words is crucial for more advanced tasks like part-of-speech tagging or chunking.  \n",
    "- While not always necessary for the Wiki or IMSDb scraping, it can be useful if your approach requires deeper text processing (e.g., ignoring certain words by part-of-speech or analyzing script lines).\n",
    "\n",
    "Feel free to experiment with NLTK’s capabilities if you find that the basic HTML parsing and regex approach **isn’t** enough for your homework.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae9b27",
   "metadata": {},
   "source": [
    "Let consider a related example to the lecture. [This](https://en.wikipedia.org/wiki/Around_the_World_in_Eighty_Days) novel contains geographical information, so lets identify all names cities in it and put these on a map. \n",
    "\n",
    "The file is not available in `nltk`, so we need to scrape it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2dbb7b",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Annotated NLP Example: Identifying City Names from a Novel, with Regex Explanation\n",
    "\n",
    "Below is **one** raw Markdown chunk containing a detailed tutorial on scraping text from Project Gutenberg, identifying city names with `nltk.ne_chunk`, verifying them on Wikipedia, and plotting them. All code fences use triple backticks with language spec; the entire content is wrapped in extra backticks so it remains raw Markdown. **Additional emphasis** is placed on explaining the usage of `re.findall(r\"\\w+\", document)`.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "In this example, we:\n",
    "\n",
    "1. **Scrape** Jules Verne’s _Around the World in 80 Days_ (Project Gutenberg ID #103).  \n",
    "2. **Extract** text from `<p>` paragraphs.  \n",
    "3. **Tokenize** using **regular expressions** and **NLTK**.  \n",
    "4. **Perform** Named Entity Recognition to find GPEs (“Geo-Political Entities”), which potentially indicates cities.  \n",
    "5. **Verify** each potential city with Wikipedia to confirm it’s actually a city.  \n",
    "6. **Fetch** latitude & longitude for each verified city.  \n",
    "7. **Plot** the results on a map with Plotly.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Key Regex Explanation\n",
    "\n",
    "- **`re.findall(r\"\\w+\", document)`**:  \n",
    "  - `\\w` is a shorthand character class in regex that matches **“word characters”** (letters, digits, and underscores by default).  \n",
    "  - The `+` quantifier means **“one or more”** of those word characters.  \n",
    "  - Thus, `r\"\\w+\"` captures **consecutive alphanumeric** “word-like” sequences while effectively discarding punctuation and whitespace.  \n",
    "  - In this example, we use `re.findall(...)` to obtain a **list of word tokens** from each paragraph before feeding them into NLTK for POS tagging and NER.\n",
    "\n",
    "This approach is a **simpler form** of tokenization: punctuation will be excluded, and words like `O'Neill` would be split at the apostrophe. If you need a more nuanced approach (keeping some punctuation, handling contractions, etc.), other **tokenizers** (e.g., `nltk.word_tokenize`) might be preferable. But `\\w+` is typically enough for quick “word-only” tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Full Code with Annotations\n",
    "\n",
    "```python\n",
    "###########################################################\n",
    "# 1. IMPORTS\n",
    "###########################################################\n",
    "import requests_cache\n",
    "from lxml.html import fromstring\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Explanation:\n",
    "# - requests_cache: to cache repeated HTTP requests (avoid re-downloading pages).\n",
    "# - lxml.html: parse HTML from strings so we can run XPath queries.\n",
    "# - re: Python's built-in regex module for text matching/splitting.\n",
    "# - nltk: Natural Language Toolkit, used here for POS tagging and Named Entity Recognition (NER).\n",
    "# - time: to add small sleep intervals (prevent hitting server rate limits).\n",
    "# - pandas, plotly.express: for data manipulation and interactive mapping.\n",
    "\n",
    "###########################################################\n",
    "# 2. SCRAPE DATA FROM PROJECT GUTENBERG\n",
    "###########################################################\n",
    "# We fetch Jules Verne’s \"Around the World in 80 Days\" from Project Gutenberg.\n",
    "s = requests_cache.CachedSession()\n",
    "r = s.get(\"https://www.gutenberg.org/files/103/103-h/103-h.htm\")\n",
    "html = fromstring(r.text)\n",
    "\n",
    "# Extract paragraphs under <div class=\"chapter\"> into a list.\n",
    "corpus = html.xpath('//div[@class=\"chapter\"]/p/text()')\n",
    "\n",
    "# Display the first paragraph for illustration.\n",
    "document = corpus[0]\n",
    "print(\"EXAMPLE PARAGRAPH:\\n\", document)\n",
    "\n",
    "###########################################################\n",
    "# 3. TOKENIZING & NAMED ENTITY RECOGNITION\n",
    "###########################################################\n",
    "# Let's see how we tokenize a single paragraph and identify GPEs (cities).\n",
    "# Step 1: Basic regex-based tokenization with \\w+ to capture word-like tokens.\n",
    "\n",
    "words = re.findall(r\"\\w+\", document)\n",
    "# Explanation:\n",
    "# \"\\w+\" = sequences of word chars (letters, digits, underscores).\n",
    "# This effectively breaks text into alphanumeric \"words\", ignoring punctuation.\n",
    "\n",
    "print(\"\\nTOKENS:\\n\", words)\n",
    "\n",
    "# Step 2: Part-of-Speech tagging\n",
    "token = nltk.pos_tag(words)\n",
    "# Example: [(\"Mr\", \"NNP\"), (\"Phileas\", \"NNP\"), (\"Fogg\", \"NNP\"), ... ]\n",
    "\n",
    "# Step 3: Named Entity Recognition using nltk.ne_chunk\n",
    "chunk = nltk.ne_chunk(token)\n",
    "# chunk is an nltk.Tree with labeled subtrees like PERSON, ORGANIZATION, GPE, etc.\n",
    "\n",
    "print(\"\\nCHUNK (NER TREE):\\n\", chunk)\n",
    "\n",
    "###########################################################\n",
    "# 4. EXTRACTING CITY CANDIDATES\n",
    "###########################################################\n",
    "# Let's store GPE-labeled entities in a global list (not best practice, but simple here).\n",
    "city_candidates = []\n",
    "\n",
    "# A function to preprocess each paragraph:\n",
    "def preprocess_paragraph(doc_text):\n",
    "    global city_candidates\n",
    "    \n",
    "    # a) Tokenize with regex\n",
    "    w = re.findall(r\"\\w+\", doc_text)\n",
    "    # b) POS tag\n",
    "    t = nltk.pos_tag(w)\n",
    "    # c) Named Entity Recognition\n",
    "    c = nltk.ne_chunk(t)\n",
    "    \n",
    "    # d) Extract GPE-labeled subtrees\n",
    "    for subtree in c:\n",
    "        if isinstance(subtree, nltk.Tree):\n",
    "            if subtree.label() == \"GPE\":\n",
    "                # Gather all tokens from that subtree\n",
    "                city_candidates.append([leaf[0] for leaf in subtree.leaves()])\n",
    "\n",
    "# Apply to every paragraph in the corpus\n",
    "for doc in corpus:\n",
    "    preprocess_paragraph(doc)\n",
    "\n",
    "# Convert the collected GPE-lists to a unique set of strings\n",
    "city_candidates = set(\" \".join(c) for c in city_candidates)\n",
    "print(\"\\nPOTENTIAL CITY CANDIDATES:\\n\", city_candidates)\n",
    "\n",
    "###########################################################\n",
    "# 5. WIKIPEDIA LOOKUP\n",
    "###########################################################\n",
    "# Check if the candidate is actually a city using Wikipedia categories.\n",
    "def check_for_city(city):\n",
    "    \"\"\"\n",
    "    Return True if 'city' has 'cities' in its Wikipedia categories.\n",
    "    If not found, tries appending ' city' (e.g., 'London city').\n",
    "    \"\"\"\n",
    "    is_city = False\n",
    "    time.sleep(0.05)\n",
    "    r = s.get(\"https://en.wikipedia.org/wiki/\" + city)\n",
    "    html = fromstring(r.text)\n",
    "\n",
    "    try:\n",
    "        categories = html.xpath('//div[@id=\"mw-normal-catlinks\"]')[0].text_content()\n",
    "    except:\n",
    "        categories = \"\"\n",
    "\n",
    "    if 'cities' in categories:\n",
    "        is_city = True\n",
    "    elif 'city' not in city:\n",
    "        # Attempt with \" city\" appended\n",
    "        is_city = check_for_city(city + \" city\")\n",
    "\n",
    "    return is_city\n",
    "\n",
    "def get_coord(city):\n",
    "    \"\"\"\n",
    "    If 'city' is recognized as a city on Wikipedia,\n",
    "    return (latitude, longitude). Otherwise return False.\n",
    "    \"\"\"\n",
    "    print(city)\n",
    "    is_city = False\n",
    "    time.sleep(0.05)\n",
    "    r = s.get(\"https://en.wikipedia.org/wiki/\" + city)\n",
    "    html = fromstring(r.text)\n",
    "\n",
    "    try:\n",
    "        categories = html.xpath('//div[@id=\"mw-normal-catlinks\"]')[0].text_content()\n",
    "    except:\n",
    "        categories = \"\"\n",
    "\n",
    "    if 'cities' in categories:\n",
    "        is_city = True\n",
    "    elif 'city' not in city:\n",
    "        # Attempt city + \" city\"\n",
    "        appended_check = get_coord(city + \" city\")\n",
    "        if not isinstance(appended_check, bool):\n",
    "            return appended_check\n",
    "\n",
    "    if is_city:\n",
    "        try:\n",
    "            lat_string = html.xpath('//span[@class=\"latitude\"]/text()')[0]\n",
    "            # e.g. \"22°18′N\" -> parse out numbers\n",
    "            lat_nums = re.findall(r'\\d+', lat_string)\n",
    "            lat = float(lat_nums[0] + '.' + lat_nums[1])\n",
    "            if 'S' in lat_string:\n",
    "                lat = -lat\n",
    "\n",
    "            long_string = html.xpath('//span[@class=\"longitude\"]/text()')[0]\n",
    "            long_nums = re.findall(r'\\d+', long_string)\n",
    "            long = float(long_nums[0] + '.' + long_nums[1])\n",
    "            if 'W' in long_string:\n",
    "                long = -long\n",
    "\n",
    "            return (lat, long)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    return False\n",
    "\n",
    "# Apply get_coord to every candidate, storing in a dict\n",
    "coord = {city: get_coord(city) for city in city_candidates}\n",
    "\n",
    "# Filter out those that returned False\n",
    "valid_cities = {k: v for k, v in coord.items() if v is not False}\n",
    "\n",
    "###########################################################\n",
    "# 6. BUILD A DATAFRAME AND PLOT\n",
    "###########################################################\n",
    "df = pd.DataFrame(valid_cities).T.reset_index()\n",
    "df = df.rename(columns={'index': 'Name', 0: 'latitude', 1: 'longitude'})\n",
    "\n",
    "# Set your Mapbox token (read from file or environment variable)\n",
    "px.set_mapbox_access_token(open(\"./../keys/mapbox.txt\").read())\n",
    "\n",
    "# Plot with Plotly\n",
    "fig = px.scatter_mapbox(df, \n",
    "                        lat='latitude', \n",
    "                        lon='longitude', \n",
    "                        hover_name=\"Name\", \n",
    "                        zoom=4)\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Key Takeaways\n",
    "\n",
    "1. **Regex Tokenization** via `\\w+`:  \n",
    "   - Finds sequences of letters/digits, effectively ignoring punctuation and whitespace.  \n",
    "   - Straightforward but may split words at contractions or miss hyphenated terms.\n",
    "\n",
    "2. **Named Entity Recognition with `nltk.ne_chunk`**:  \n",
    "   - After POS-tagging, `ne_chunk` identifies labeled subtrees (e.g., `PERSON`, `ORGANIZATION`, `GPE`).  \n",
    "   - We specifically look for `GPE` as a clue to potential city or country names.\n",
    "\n",
    "3. **Wikipedia Verification**:  \n",
    "   - Avoid false positives by checking whether a candidate has `'cities'` in its category links.\n",
    "\n",
    "4. **Coordinate Extraction**:  \n",
    "   - Parse `<span class=\"latitude\">` and `<span class=\"longitude\">` from Wikipedia’s infobox.  \n",
    "   - Convert them into floating-point coordinates.\n",
    "\n",
    "5. **Interactive Map**:  \n",
    "   - With **Plotly**, we can visualize these recognized cities around the world.\n",
    "\n",
    "``` \n",
    "````````markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06879f",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# **STA 141B WQ 25 Homework 3 (HINTS / PSEUDO-CODE VERSION)**\n",
    "\n",
    "Below are **hints** and **partial/pseudo-code** for the given exercises. **No full solutions** are provided—only pointers on **logic** and **key methods/libraries**. You’re expected to fill in the details yourself.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 2: UC Irvine Compensation Data**\n",
    "\n",
    "**Goal**: Retrieve UC Irvine (UCI) *professor* compensation data from UC’s open compensation site for 2023, then parse department info from the UCI directory.\n",
    "\n",
    "### (a) Getting Compensation Data\n",
    "\n",
    "1. **Send POST Request**  \n",
    "   - Use `requests.post(...)` to query the UC compensation API (e.g., `https://ucannualwage.ucop.edu/wage/search.do`).\n",
    "   - Provide parameters: e.g. `year=2023`, `title='PROF'`, `location='Irvine'`, etc.  \n",
    "2. **Check/Parse Response**  \n",
    "   - Use `.json()` to convert the response to Python structures.\n",
    "   - Inspect `data['rows']` for the returned entries.  \n",
    "   - Count with `len(...)`.\n",
    "\n",
    "#### **Pseudo-Code Sketch**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# Example pattern (fill in correct params/URL)\n",
    "result = requests.post(\"API_URL\", params={\n",
    "    'year': 2023, \n",
    "    'title': 'PROF',\n",
    "    'location': 'Irvine',\n",
    "    # other params...\n",
    "})\n",
    "data = result.json()\n",
    "all_entries = data['rows']  # might hold your records\n",
    "\n",
    "print(len(all_entries))  # total returned\n",
    "```\n",
    "\n",
    "**Hint**: Some fields may appear as `\"*****\"`. You can filter those if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Department Data via UCI Directory\n",
    "\n",
    "1. **Extract Name Info**  \n",
    "   - For each record, pick out first name/last name.  \n",
    "2. **Call the UCI Directory**  \n",
    "   - Possibly `requests.post(\"https://directory.uci.edu/render-list\", data={...})` or similar.  \n",
    "   - Provide a string like `\"FirstName LastName\"` in the form data.  \n",
    "3. **Parse HTML**  \n",
    "   - Use `lxml.html` or similar to parse the returned page.  \n",
    "   - Extract the department text if available (e.g., an XPath like `//td[strong[text()=\"Department\"]]/...`).  \n",
    "4. **Store in a DataFrame**  \n",
    "   - Convert your data into a `pandas.DataFrame`.  \n",
    "   - Convert numeric pay columns with `pandas.to_numeric(...)`.  \n",
    "5. **Group & Aggregate**  \n",
    "   - `df.groupby(\"department\").mean()` or `.agg(...)`.  \n",
    "   - Sort by `gross` or `base` to find top departments.\n",
    "\n",
    "#### **Pseudo-Code Sketch**\n",
    "\n",
    "```python\n",
    "import lxml.html as lx\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "departments = []\n",
    "for record in all_entries:\n",
    "    # 1) Extract first/last name\n",
    "    # 2) Directory request\n",
    "    # 3) Parse HTML, find \"Department\" text\n",
    "    # 4) Append to departments list\n",
    "    sleep(0.01)  # minimal delay to be polite to server\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'gross': [...], \n",
    "    'base': [...],\n",
    "    'department': departments\n",
    "})\n",
    "\n",
    "# Convert to numeric\n",
    "df['gross'] = pd.to_numeric(df['gross'])\n",
    "df['base']  = pd.to_numeric(df['base'])\n",
    "\n",
    "grouped = df.groupby('department').mean()\n",
    "top_by_gross = grouped.sort_values('gross', ascending=False).head(4)\n",
    "top_by_base  = grouped.sort_values('base',  ascending=False).head(4)\n",
    "```\n",
    "\n",
    "**Hint**: You might find 500–1000 records with valid department data (some names may not match).\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 2 (Wiki Game Variation)**\n",
    "\n",
    "**Goal**: Start from a random (or specified) Wikipedia article, follow the **first non-italicized link** outside parentheses/infobox. Stop under any of these conditions:\n",
    "\n",
    "- Reaches `/wiki/Philosophy`\n",
    "- Finds a dead end (no valid links)\n",
    "- Loops (revisiting a page already in the path)\n",
    "\n",
    "### 1. Removing Parentheses\n",
    "\n",
    "- Create a function using `re.sub(...)` to remove `( ... )` sections from text repeatedly.\n",
    "- Ensure you only remove plain parentheses text, not parts of link markup.\n",
    "\n",
    "### 2. Skipping Special / External Links\n",
    "\n",
    "- Validate link with a regex like `^/wiki/` but exclude `/wiki/File:`, `/wiki/Category:`, `/wiki/Special:`.  \n",
    "- Also skip external links (`http://`, `https://`).\n",
    "\n",
    "### 3. Gathering Articles in a Chain\n",
    "\n",
    "- Start from either `Special:Random` or a given `article` link.\n",
    "- **Fetch** the page, **parse** main content, **remove** parentheses if needed.\n",
    "- **Pick** the first valid link **not** in italic or an excluded region (infobox, note boxes, etc.).\n",
    "- If no valid link, return `None`.\n",
    "\n",
    "#### **Pseudo-Code Sketch**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import lxml.html as lx\n",
    "import re\n",
    "import time\n",
    "\n",
    "def remove_parenthesis(text):\n",
    "    # repeatedly use re.sub to remove ( ... )\n",
    "    # return cleaned text\n",
    "\n",
    "def check_link(link):\n",
    "    # verify link starts with /wiki/\n",
    "    # exclude /wiki/File: etc.\n",
    "\n",
    "def get_article(article_link):\n",
    "    # 1) request: \"https://en.wikipedia.org\" + article_link\n",
    "    # 2) remove parentheses from text\n",
    "    # 3) parse valid <a> from the main content\n",
    "    # 4) pick the first passing check_link\n",
    "    # 5) return that link or None\n",
    "\n",
    "def play(start_article=None):\n",
    "    visited = []\n",
    "    # if no start_article => use Special:Random\n",
    "    current_article = ...\n",
    "    while True:\n",
    "        visited.append(current_article)\n",
    "        next_link = get_article(current_article)\n",
    "        if (no more links) or (loop) or (Philosophy):\n",
    "            break\n",
    "        current_article = next_link\n",
    "    \n",
    "    # add last link\n",
    "    visited.append(next_link)\n",
    "    return visited\n",
    "```\n",
    "\n",
    "**Hint**: Use an **XPath** to exclude `<i>`, `<table>`, `<figure>`, or note-like `<div>`s from your search for the first link.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Running 200 Times + Collecting Stats\n",
    "\n",
    "- Call `play()` in a loop (200 runs).  \n",
    "- **Count** how many end with `/wiki/Philosophy`.  \n",
    "- **Compute** average and max path lengths.  \n",
    "- **Collect** all visited articles to find the top 10 most visited (`collections.Counter` or `pandas.value_counts`).  \n",
    "- **Unique** article count is `len(set(...))`.\n",
    "\n",
    "#### **Pseudo-Code Sketch**\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "results = []\n",
    "for i in range(200):\n",
    "    chain = play()\n",
    "    results.append(chain)\n",
    "\n",
    "# (i) how many end in Philosophy\n",
    "count_philos = sum(1 for c in results if c[-1] == '/wiki/Philosophy')\n",
    "\n",
    "# (ii) average length\n",
    "avg_len = sum(len(c) for c in results) / 200\n",
    "\n",
    "# (iii) maximum length\n",
    "max_len = max(len(c) for c in results)\n",
    "\n",
    "# (iv) ten most often visited\n",
    "all_articles = [art for chain in results for art in chain]\n",
    "counts = Counter(all_articles)\n",
    "most_common_10 = counts.most_common(10)\n",
    "\n",
    "# (v) number of distinct visited articles\n",
    "unique_count = len(set(all_articles))\n",
    "```\n",
    "\n",
    "### 5. Starting from Philosophy\n",
    "\n",
    "- If you want the chain starting at `/wiki/Philosophy`, do `play(\"/wiki/Philosophy\")`.\n",
    "- Print/inspect the list.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Notes**\n",
    "\n",
    "These hints cover **key steps** (HTTP requests, regex filtering, HTML parsing, data aggregation) but **do not provide** the full working solution. You must fill in details and handle edge cases (like missing fields, unexpected HTML structures, etc.). Good luck!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cf1adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
