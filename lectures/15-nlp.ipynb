{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9853b96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA 141B Data & Web Technologies for Data Analysis\n",
    "\n",
    "### Lecture 15, 2/26/25, Natural language processing - LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944541d7",
   "metadata": {},
   "source": [
    "### Announcements \n",
    "\n",
    "- Homework assignment 4 has been published. The due date has been changed: It's March 9th. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7598ccc",
   "metadata": {},
   "source": [
    "### Last week's topics\n",
    "- Natural language processing\n",
    "    - Tokenizing\n",
    "    - Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b303a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Today's topics\n",
    "- Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40579410",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a probabilistic model for a collection of discrete data. It is motivated to analyise texts. The discrete data consiststs on *words*, that are contain in *documents*, which in turn are collected in the *corpus*. Each document contains differents *topics*, which we are interested in. The topics are not observable, i.e., latent, and have to be estimated. \n",
    "\n",
    "- *word* is the smallest entity (token, term) of the discrete data and an element from the dictionary of $D$ words. The $i$-th word of the dictionary is a vector with zero entries except on position $i$. \n",
    "- *documents* are a series of $N$ words, denoted as $W = (w_1, \\dots, w_N)$. \n",
    "- *corpus* is the collection of $M$ documents, $\\{W_1, \\dots, W_M\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa0853",
   "metadata": {},
   "source": [
    "### References\n",
    " - [Latent Dirichlet Analysis](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107f741",
   "metadata": {},
   "source": [
    "A LDA is modelled via a Bayesian hierarchy. Before we state it, we have to restate some distributions. \n",
    "\n",
    "##### Multinomial\n",
    "\n",
    "Let $n,k\\in\\{0,1,2,...\\}$ and $p = (p_1, \\dots, p_k)'\\in[0,1]^k$ with $\\sum_{i=1}^kp_i=1$. $X\\sim Mult(n,p)$ if \n",
    "$$\n",
    "P(X_1=x_1, \\dots, X_k = x_k) = \\begin{cases} \n",
    "\\frac{n!}{x_1\\cdots x_k!}p_1^{x_1}\\cdots p_k^{x_k}& \\text{if}\\sum_{i=1}^kx_i = n,\\\\\n",
    "0, &\\text{else.}\n",
    "\\end{cases}\n",
    "$$\n",
    "The Multinomial distribution is a generalization of the Binomial distribution for the case of more than two results. \n",
    "\n",
    "##### Dirichlet \n",
    "\n",
    "The Dirichlet distribution is a generalization of the Beta distribution. \n",
    "A Dirichlet distribution of order $k\\geq2$ and parameters $\\alpha_1, \\dots, \\alpha_k>0$ has the p.d.f. \n",
    "$$\n",
    "f(p_1,\\dots, p_k) = \\frac{\\Gamma(\\sum_{i=1}^k\\alpha_i)}{\\prod_{i=1}^k\\Gamma(\\alpha_i)}\\prod_{i=1}^kp_i^{\\alpha_i-1}\n",
    "$$\n",
    "for all $p_1, \\dots, p_k\\geq0$ with $\\sum_{i=1}^kp_i=1$. This implies that the $p_i$ lie on a $k-1$-dimensional simplex. For $k=2$, the Dirichlet distribution coincides with the Beta distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e1271",
   "metadata": {},
   "source": [
    "An important propery of the Dirichlet distribution is that its a conjugate prior to the Multiomial distribution. Consider the following hierarchy: \n",
    "\\begin{align}\n",
    "X|p&\\sim Mult(n,p)\\\\\n",
    "p&\\sim Dir(\\alpha)\n",
    "\\end{align}\n",
    "The parameter $p$ is not assumed to be fixed (as in an frequentist understanding) but random. Consequently, it cannot be estimated. The Bayesian methodology aims in computing the posterior distribution of $p$, given the observed data $X$:\n",
    "\\begin{align}\n",
    "P(p|X) = \\frac{P(X|p)P(p)}{P(X)}\n",
    "\\end{align}\n",
    "One can show that the posterior probabiliy is again a Dirichlet distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71523a8",
   "metadata": {},
   "source": [
    "Back to the data! We assume that every document is a collection of latent topics. We can learn about a topic by the distribution of words in the document. For each document, we assume the data generating process: \n",
    "\n",
    "- Choose $\\theta\\sim Dir(\\alpha)$, where $\\theta, \\alpha\\in\\mathbb{R}^k$. \n",
    "- For each word $w_i$ in the document $W$, $i = 1, \\dots, N$: \n",
    "    - Choose a topic $z_i\\sim Mult(1,\\theta)$,\n",
    "    - Choose a word from $P(w_i|z_i,\\beta)$. \n",
    "        \n",
    "Here, $P(w_i|z_i,\\beta)$ is a multinomial probability with $n=1$, given topic $z_i$. We assume that the number of topics $k$ is known and will not be modelled as random for now. The parameter $\\beta\\in\\mathbb{R}^{k\\times D}$, and $\\beta_{ij}$ is the probability that word $j$ is chosen for topic $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14603271",
   "metadata": {},
   "source": [
    "##### Example\n",
    "\n",
    "Consider as corpus a magazine about housekeeping. This magazine consists of three articles (documents), and each article consists of the topics `home`, `garden`, `cooking` and the words `pan`, `plot`, `window` and `way`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.array(['home', 'garden', 'cooking'])\n",
    "dictionary = np.array(['pan', 'plot', 'window',  'way'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85eab84",
   "metadata": {},
   "source": [
    "For parameters $\\alpha_i$, the parameter $\\theta$ determines the probability of topics in each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [1, 2, 3]\n",
    "theta = np.random.dirichlet(alpha)\n",
    "theta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65aace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zi = np.random.multinomial(1, theta) # topic for word \n",
    "zi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.array([[0.1, 0.02, 0.88, 0],\n",
    "                 [0.01, 0.79, 0.1, 0.1],\n",
    "                 [0.75, 0.15, 0.1, 0]])\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ede0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[zi==1,][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.multinomial(1,beta[zi==1,][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zi = np.random.multinomial(1, theta) # topic for word \n",
    "wi = np.random.multinomial(1, beta[zi==1,][0]) \n",
    "\n",
    "print({'topic': topics[zi==1][0]})\n",
    "print({'word': dictionary[wi==1][0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7f518",
   "metadata": {},
   "source": [
    "The complete corresponding hierarchy is given as \n",
    "\\begin{align}\n",
    "    w_i|z_i,\\beta_i&\\sim Mult(1,\\beta_i)\\\\\n",
    "    z_i|\\theta &\\sim Mult(1,\\theta)\\\\\n",
    "    \\theta&\\sim Dir(\\alpha)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65b2f5",
   "metadata": {},
   "source": [
    "If $\\beta_i$ and $\\alpha$ are known, the posterior probability of $\\theta, z_i|w_i, \\alpha, \\beta_i$ could be approximated using MCMC methods. The method of empirical Bayes uses estimates of $\\beta_i$ and $\\alpha$ to do so:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee5ec5",
   "metadata": {},
   "source": [
    "Note that the joint distribution for a single word is given by \n",
    "$$P(\\theta, z_i, w_i|\\alpha_i, \\beta_i) = P(w_i| z_i, \\beta_i)P(z_i|\\theta)P(\\theta|\\alpha), $$\n",
    "and for a single document (let $Z = (z_1, \\dots, z_N)$) thus by \n",
    "$$P(\\theta, Z,W|\\alpha, \\beta) = \\prod_{i=1}^N P(w_i| z_i, \\beta_i)P(z_i|\\theta)P(\\theta|\\alpha) = P(\\theta|\\alpha) \\prod_{i=1}^N P(w_i| z_i, \\beta_i)P(z_i|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c38362",
   "metadata": {},
   "source": [
    "Integrating over $\\theta$ and summing over $Z$ gives the marginal distribution over this document: \n",
    "$$\n",
    "P(W|\\alpha, \\beta) \n",
    "= \n",
    "\\int P(\\theta|\\alpha) \\prod_{i=1}^N \\sum_{z_i}P(w_i| z_i, \\beta_i)P(z_i|\\theta) d\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d46a9",
   "metadata": {},
   "source": [
    "The marginal distribution for the corpus $C$ is thus given as \n",
    "$$\n",
    "P(C|\\alpha, \\beta) \n",
    "= \n",
    "\\prod_{j=1}^M \\int P(\\theta|\\alpha) \\prod_{i=1}^N \\sum_{z_{ij}}P(w_i| z_{ij}, \\beta_i)P(z_{ij}|\\theta) d\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009af7e",
   "metadata": {},
   "source": [
    "Natural estimators for $\\alpha$ and $\\beta$ are found by solving \n",
    "$$\n",
    "\\max_{\\alpha, \\beta} P(C|\\alpha, \\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc31c28",
   "metadata": {},
   "source": [
    "![LDAnormal](./source/LDAnormal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be0091",
   "metadata": {},
   "source": [
    "Problematically, if the vocabulary is large enough, it is likely of encountering a new word. Using EB, those words would receive probability zero. The usual approach in such cases is to smooth the underlying probability distribution for the words: \n",
    "\n",
    "\\begin{align}\n",
    "    w_i|z_i,\\beta_i&\\sim Mult(1,\\beta_i)\\\\\n",
    "    \\beta_i|\\eta &\\sim Dir(\\eta)\\\\\n",
    "    z_i|\\theta &\\sim Mult(1,\\theta)\\\\\n",
    "    \\theta&\\sim Dir(\\alpha)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b62b5",
   "metadata": {},
   "source": [
    "The hyperparameters $\\alpha$ and $\\eta$ are estimated using EB as outlined above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd771b",
   "metadata": {},
   "source": [
    "![LDAnormal](./source/LDAsmooth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f4302c",
   "metadata": {},
   "source": [
    "#### [&#128011;](https://www.gutenberg.org/files/2701/2701-h/2701-h.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby = nltk.corpus.gutenberg.raw(\"melville-moby_dick.txt\")\n",
    "pattern = r\"\\s*(?:EXTRACTS|ETYMOLOGY\\.|Epilogue|CHAPTER \\d+)\\s+.+\\n*.+[\\.!\\?\\)]\\s*\"\n",
    "corpus = re.split(pattern, moby)\n",
    "corpus.pop(0)\n",
    "corpus = [re.sub(r\"\\s+\", \" \", document).lower() for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d98609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus[26][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc2ba5",
   "metadata": {},
   "source": [
    "From [here](https://digitalcommons.cwu.edu/cgi/viewcontent.cgi?article=1430&context=etd#:~:text=In%20Chapters%201%2D8%2C%2010,role%20of%20%22I%22%20narrator.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38060672",
   "metadata": {},
   "outputs": [],
   "source": [
    "cetological = [0, 1, 46, 54, 59, 60, 61, 63, 64, 68, 75, \n",
    "               76, 77, 78, 80, 81, 85, 86, 87, 89, 90, 91, \n",
    "               93, 96, 97, 98, 102, 103, 104, 105, 106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = [i not in cetological for i in range(138)]\n",
    "labels = [i for i in map(lambda x: 'story' if x else 'ceto', story)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = [nltk.word_tokenize(document) for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords.extend([',', '.', ':', '!', ';', '?', '--', '\\'', '\\'\\''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a546b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = [[word for word in document if word not in stopwords] for document in corpus_tokenized] # remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = [[nltk.PorterStemmer().stem(word) for word in document] for document in corpus_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [' '.join(document) for document in corpus_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99213c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[30][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb5b39c",
   "metadata": {},
   "source": [
    "Now, we will use the [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). \n",
    "\n",
    "We will have to reshape our corpus into a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a21cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(tokenizer = nltk.word_tokenize)\n",
    "freq = vec.fit_transform(corpus)\n",
    "corpus_freq = freq.todense() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bfaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_freq = np.array(corpus_freq) # removes warning further down\n",
    "corpus_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2191a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb8de96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_freq.shape # some words less than when checked last time, due to stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics = 2\n",
    "lda = LatentDirichletAllocation(n_components = ntopics, \n",
    "                                learning_method = 'online', \n",
    "                               random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(corpus_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cb30c",
   "metadata": {},
   "source": [
    "Now what? Lets investigate if the topic distribution changes over the chapters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = lda.transform(corpus_freq)\n",
    "posterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d60aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b930236",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5dcc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(posterior).reset_index()\n",
    "df.columns = ['chapter'] + ['Topic ' + str(i + 1) for i in range(0,ntopics)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd641cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.melt(df, id_vars = 'chapter')\n",
    "ddf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e12229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.line(ddf, x=\"chapter\", y=\"value\", color='variable', labels={\n",
    "                     \"chapter\": \"Chapter\",\n",
    "                     \"value\": \"Probability\",\n",
    "                     \"variable\": \"LDA Topics\"\n",
    "                 }, title = 'LDA Probabilities: Labelled cetology chapters are shaded')\n",
    "\n",
    "for i, e in enumerate(labels): \n",
    "    if e=='ceto': \n",
    "        fig.add_vrect(x0=i-0.5, x1=i+0.5, line_width=0, fillcolor=\"grey\", opacity=0.2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6b970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df[(df['Topic 1']>0.335).values] # chapters of topic 1 - change cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6743c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lda.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f841ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTopics = pd.DataFrame(lda.components_.T, index = vec.get_feature_names_out())\n",
    "wordTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTopics = wordTopics.apply(lambda x: x / sum(x), 1)\n",
    "wordTopics.columns = ['Topic ' + str(i + 1) for i in range(0,ntopics)]\n",
    "wordTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373490a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTopics['Topic 1'].sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae93e30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordTopics['Topic 2'].sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b843023",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTopics['Topic 3'].sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b24aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordTopics['Topic 4'].sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f4b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd635189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2b12ec50b0a525a62abe739e766b0c808eccd181e3f804cedbbca00f4d5b392"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
