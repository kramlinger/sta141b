{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38eee13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA 141B Data & Web Technologies for Data Analysis\n",
    "\n",
    "### Lecture 16, 3/4/25, Natural language processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb3313",
   "metadata": {},
   "source": [
    "### Announcements \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c68faf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Today's topics\n",
    "- Chunking\n",
    "    - Noun Phrase Chunking\n",
    "    - Tag Patterns\n",
    "    - Developing and Evaluating Chunkers\n",
    "    - Chinking\n",
    "- Training Classifier-Based Chunkers\n",
    "- Cascaded Chunker\n",
    "- Named Entity Recognition\n",
    "- Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dcbd5b",
   "metadata": {},
   "source": [
    "## The Bag-of-words Model\n",
    "\n",
    "The one-hot encoding, term frequencies, and TF-IDF scores all ignore word order.\n",
    "\n",
    "The _bag-of-words model_ assumes that the order of words in a document doesn't matter. Imagine taking the words in each document and dumping them into a bag, where they get all mixed up. Note that in this case \"model\" means a way of thinking about a problem, not a statistical model.\n",
    "\n",
    "While the order of words in a document might seem important, the bag-of-words model is surprisingly useful. The bag-of-words model is a good place to start if you want to use statistical methods on language data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e68d0d",
   "metadata": {},
   "source": [
    "#### N-Grams\n",
    "\n",
    "So far we have tagged unigrams. But they ignore word order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b66af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The cat saw the dog was angry at the other cat.\", \n",
    "          \"The dog saw the cat was angry at the other cat.\", \n",
    "          \"The canary saw the iguana was sad.\", \n",
    "          \"They refuse to permit us to obtain the refuse permit.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [[word.lower() for word in nltk.word_tokenize(document)] for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f97c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i for i in nltk.bigrams(words[0])]\n",
    "b = [i for i in nltk.bigrams(words[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a539eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c065622",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(a).intersection(set(b))) / len(set(a)) #bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49e6af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(set(words[0]).intersection(set(words[0]))) / len(set(words[0])) #unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee634a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(words[0]) == set(words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [i for i in nltk.trigrams(words[0])]\n",
    "e = [i for i in nltk.trigrams(words[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00920e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(d).intersection(set(e))) / len(set(d)) #bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5395db",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [i for i in nltk.ngrams(words[0], 4)]\n",
    "d = [i for i in nltk.ngrams(words[1], 4)]\n",
    "\n",
    "set(d).intersection(set(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6aa554",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2921ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(doc): \n",
    "    words = [word.lower() for word in nltk.word_tokenize(doc)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb57083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(tokenizer = myTokenizer)\n",
    "tfidf = vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfdf=pd.DataFrame(tfidf.todense().T)\n",
    "#tfidfdf.set_index(pd.Series([tuple(t) for t in vec.get_feature_names_out()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b146ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tfidf @ tfidf.T).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44eba28",
   "metadata": {},
   "source": [
    "### Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b211ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf457d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['canary', 'saw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fe3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf4a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = words[3].copy()\n",
    "np.random.shuffle(w)\n",
    "nltk.pos_tag(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a5a7c",
   "metadata": {},
   "source": [
    "Although the some taggers tagged unigrams, they are not all unigram taggers: `nltk.pos_tag` considers the position of the token relative to other tokens. In contrast, `regexp_tagger` (which a set of rules that we establish) is a unigram tagger. \n",
    "\n",
    "An n-gram tagger is a generalization of a unigram tagger whose context is the current word together with the part-of-speech tags of the n-1 preceding tokens.\n",
    "\n",
    "We can train taggers on training data. Some tagged training data is available in `nltk`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3615947",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256728ab",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import lxml.html as lx\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b175cbf",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def get_info(country):\n",
    "    time.sleep(0.2)\n",
    "    result = requests.get(\n",
    "        'https://www.cia.gov/the-world-factbook/page-data/countries/' \n",
    "                          + country + '/page-data.json')\n",
    "    result.raise_for_status()\n",
    "    return result.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_info(\"burma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [i for i in data['result']['data']['fields']['nodes'] \\\n",
    "            if i.get('name') == 'Background'][0]['data']\n",
    "document = \"\".join([t for t in lx.fromstring(document).xpath('//p/text()')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7a096",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document):\n",
    "    #document = document.lower()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e1a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_document = preprocess(document)\n",
    "sentence = processed_document[0]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e726a45",
   "metadata": {},
   "source": [
    "#### Noun Phrase Chunking\n",
    "\n",
    "We will begin by considering the task of noun phrase chunking, or NP-chunking,\n",
    "where we search for chunks corresponding to individual noun phrases.\n",
    "\n",
    "One of the most useful sources of information for NP-chunking is part-of-speech tags. This is one of the motivations for performing part-of-speech tagging in our information extraction system. In order to create an NP-chunker, we will first define a chunk grammar, consisting of rules that indicate how sentences should be chunked.\n",
    "\n",
    "We will define a simple grammar with a single regular expression rule . This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). Using this grammar, we create a chunk parser, and test it on our example sentence . The result is a tree, which we can either print, or display graphically ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b142fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN.*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17bbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cp.parse(sentence)\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779126ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a258db0",
   "metadata": {},
   "source": [
    "#### Tag Patterns\n",
    "\n",
    "The rules that make up a chunk grammar use tag patterns to describe sequences of\n",
    "tagged words. A tag pattern is a sequence of part-of-speech tags delimited using angle\n",
    "brackets, e.g.,`<DT>?<JJ>*<NN>`. Tag patterns are similar to regular expression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ.*>*<NN.*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d93467",
   "metadata": {},
   "source": [
    "This will chunk any sequence of tokens beginning with an optional determiner, followed by zero or more adjectives of any type, followed by one or more nouns of any type. \n",
    "\n",
    "To find the chunk structure for a given sentence, the RegexpParser chunker begins with\n",
    "a flat structure in which no tokens are chunked. The chunking rules are applied in turn,\n",
    "successively updating the chunk structure. Once all of the rules have been invoked, the\n",
    "resulting chunk structure is returned.\n",
    "\n",
    "The next example shows a simple chunk grammar consisting of two rules. The first rule\n",
    "matches an optional determiner or possessive pronoun, zero or more adjectives, then a noun. The second rule matches one or more proper nouns. We also define an example\n",
    "sentence to be chunked , and run the chunker on this input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|P.*>?<JJ>*<NN>+} # chunk determiner/possessive, adjectives and nouns\n",
    "    {<NNP>+} # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "example = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"),\n",
    "            (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), \n",
    "           (\"hair\", \"NN\")]\n",
    "cp.parse(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd80a30",
   "metadata": {},
   "source": [
    "If a tag pattern matches at overlapping locations, the leftmost match takes precedence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c1a34",
   "metadata": {},
   "source": [
    "Sometimes it is easier to define what we want to exclude from a chunk. We can define a chink to be a sequence of tokens that is not included in a chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ddc35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\" NP:\n",
    "    {<.*>+}        # Chunk everything\n",
    "    }<CC|.*DT|TO>?<\\.|,|VB.*>+<IN|TO>?{  # Chink \n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00aa70",
   "metadata": {},
   "source": [
    "As befits their intermediate status between tagging and parsing, chunk\n",
    "structures can be represented using either tags or trees. The most widespread file representation\n",
    "uses IOB tags. In this scheme, each token is tagged with one of three special\n",
    "chunk tags, I (inside), O (outside), or B (begin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa6d84",
   "metadata": {},
   "source": [
    "#### Developing and Evaluating Chunkers\n",
    "Now you have a taste of what chunking does, but we haven’t explained how to evaluate\n",
    "chunkers. As usual, this requires a suitably annotated corpus. We begin by looking at\n",
    "the mechanics of converting IOB format into an NLTK tree, then at how this is done\n",
    "on a larger scale using a chunked corpus. We will see how to score the accuracy of a\n",
    "chunker relative to a corpus, then look at some more data-driven ways to search for\n",
    "NP chunks. Our focus throughout will be on expanding the coverage of a chunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it works like this \n",
    "print(cp.accuracy((result,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480557b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "18 / len(sentence) # 18 tokens have been correctly classified in terms of IOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1bd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f272f0",
   "metadata": {},
   "source": [
    "Using the corpora module we can load the data `conll2000` that has been tagged\n",
    "then chunked using the IOB notation. The chunk categories provided in this corpus\n",
    "are NP, VP, and PP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923da486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "conll2000.chunked_sents('train.txt')[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b52226",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conll2000.chunked_sents('train.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d19c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(i) for i in conll2000.chunked_sents('train.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(\"\") # we are not providing any grammar!\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d08e05",
   "metadata": {},
   "source": [
    "Now let’s try a naive regular expression chunker that\n",
    "looks for tags beginning with letters that are characteristic of noun phrase tags (e.g.,\n",
    "`CD` (cardinal number), `DT`, and `JJ`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f32a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "#test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0226edc",
   "metadata": {},
   "source": [
    "We could improve on it\n",
    "by adopting a more data-driven approach, where we use the training corpus to find the\n",
    "chunk tag (I, O, or B) that is most likely for each part-of-speech tag. In other words, we\n",
    "can build a chunker using a unigram tagger. But rather than trying to\n",
    "determine the correct part-of-speech tag for each word, we are trying to determine the\n",
    "correct chunk tag, given each word’s part-of-speech tag. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e776b83",
   "metadata": {},
   "source": [
    "### Cascaded Chunks\n",
    "\n",
    "So far, our chunk structures have been relatively flat. Trees consist of tagged tokens,\n",
    "optionally grouped under a chunk node such as NP. However, it is possible to build\n",
    "chunk structures of arbitrary depth, simply by creating a multistage chunk grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9519b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|JJ>*<NN.*>+} # Chunk sequences of DT, JJ, NN (noun phrase)\n",
    "    PP: {<IN><NP>} # Chunk prepositions followed by NP (prepositional phrase)\n",
    "    VP: {<VB.*><NP|PP>+} # Chunk verbs and their arguments (verb phrase)\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48c32a",
   "metadata": {},
   "source": [
    "This solution is not perfect, as `NP` are needlessly split. We will refine our grammar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b95064",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|JJ|NN.*>+} # Chunk sequences of DT, JJ, NN (noun phrase)\n",
    "    PP: {<IN><NP>} # Chunk prepositions followed by NP (prepositional phrase)\n",
    "    VP: {<VB.*><NP|PP>+$} # Chunk verbs and their arguments (verb phrase)\n",
    "    NP: {<NP|PP><CC>?<NP|PP>}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e0ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar, loop = 2)\n",
    "cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0edac",
   "metadata": {},
   "source": [
    "Recall: The left side takes precedence when assigning the chunks! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233da6c8",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "Named entities are definite noun phrases that refer to specific types of individuals, such as organizations,\n",
    "persons, dates, and so on.\n",
    "\n",
    "| NAMED ENTITY | EXAMPLE | \n",
    "| ---- | ---- |\n",
    "| ORGANIZATION | Georgia-Pacific Corp., WHO |\n",
    "| PERSON | Eddy Bonte, President Obama |\n",
    "| LOCATION | Murray River, Mount Everest |\n",
    "| DATE | June, 2008-06-29 |\n",
    "| TIME | two fifty a m, 1:30 p.m. |\n",
    "| FACILITY | Washington Monument, Stonehenge |\n",
    "| GEO-POLITICAL ENTITIES | South East Asia, Midlothian |\n",
    "\n",
    "The goal of a named entity recognition (NER) system is to identify textual mentions\n",
    "of the named entities. This can be broken down into two subtasks: identifying\n",
    "the boundaries of the NE, and identifying its type. \n",
    "\n",
    "How do we go about identifying named entities? One option would be to look up each\n",
    "word in an appropriate list of names. However, this is prone to errors caused by the fact that many named entity terms\n",
    "are ambiguous.\n",
    "\n",
    "Named entity recognition is a task that is well suited to the type of classifier-based\n",
    "approach that we saw for noun phrase chunking. In particular, we can build a tagger\n",
    "that labels each word in a sentence using the IOB format, where chunks are labeled by\n",
    "their appropriate type.\n",
    "\n",
    "NLTK provides a classifier that has already been trained to recognize named entities,\n",
    "accessed with the function `nltk.ne_chunk()`. If we set the parameter `binary=True`,\n",
    "then named entities are just tagged as `NE`; otherwise, the classifier adds category labels\n",
    "such as `PERSON`, `ORGANIZATION`, and `GPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67426897",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get('https://plato.stanford.edu/entries/liberalism-latin-america/')\n",
    "html=lx.fromstring(r.text)\n",
    "d=\" \".join(html.xpath('//div[@id=\"aueditable\"]//p//text()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba87705",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4180cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "def preprocess(document):\n",
    "    document = re.sub(\"\\s+\", \" \", document)\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf263c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=preprocess(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b929dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54528e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[nltk.ne_chunk(sentence) for sentence in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b26004",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89160b99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sent in t:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\"):\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8afdc8",
   "metadata": {},
   "source": [
    "### Relation Extraction\n",
    "\n",
    "Once named entities have been identified in a text, we then want to extract the relations\n",
    "that exist between them. We will typically be looking for relations\n",
    "between specified types of named entity. One way of approaching this task is to initially\n",
    "look for all triples of the form `(X, α, Y)`, where `X` and `Y` are named entities of the required\n",
    "types, and `α` is the string of words that intervenes between `X` and `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in t:\n",
    "    for i, chunk in enumerate(sent):\n",
    "        try: \n",
    "            if (hasattr(sent[i], \"label\") and sent[i].label()==\"PERSON\" \n",
    "                and sent[i+1][1]=='IN' \n",
    "                and hasattr(sent[i+2], \"label\") and sent[i+2].label()==\"GPE\"): \n",
    "                relation = {\n",
    "                    \" \".join([i[0] for i in sent[i].leaves()]): \n",
    "                    \" \".join([i[0] for i in sent[i+2].leaves()])\n",
    "                }\n",
    "                print(relation)\n",
    "        except: \n",
    "            pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b7b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
