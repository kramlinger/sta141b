{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f2c4e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA 141B Data & Web Technologies for Data Analysis\n",
    "\n",
    "### Lecture 12, 2/18/25, Natural language processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16cbae",
   "metadata": {},
   "source": [
    "### Announcements \n",
    "\n",
    "- HW 2 is graded\n",
    "- HW 3 is due this Sunday\n",
    "- Form groups for the final project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48de97",
   "metadata": {},
   "source": [
    "### Last week's topics\n",
    "\n",
    "- Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179874bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Today's topics\n",
    "- Natural Language Processing\n",
    "     - `nltk` \n",
    "     package\n",
    "     - Tokenization\n",
    "     - Regular Expressions\n",
    "     - Standardizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f584c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ressources\n",
    "- [Natural Language Processing with Python][nlpp], chapters 1-3. Beware: the print version is for Python 2.\n",
    "- [Scikit-Learn Documentation][skl], especially the section about [Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "\n",
    "[PDSH]: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "[ProGit]: https://git-scm.com/book/\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "[atap]: https://search.library.ucdavis.edu/primo-explore/fulldisplay?docid=01UCD_ALMA51320822340003126&context=L&vid=01UCD_V1&search_scope=everything_scope&tab=default_tab&lang=en_US\n",
    "[skl]: https://scikit-learn.org/stable/documentation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5fdb7",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "\n",
    "A _natural language_ is a language people use to communicate, like English, Spanish, or Mandarin. These languages evolved over thousands of years and do not have simple, explicit rules.\n",
    "\n",
    "_Natural language processing_ (NLP) means using a computer to analyze, manipulate, or synthesize natural language. Some examples of NLP tasks are:\n",
    "* Translating from one language to another\n",
    "* Recognizing speech or handwriting\n",
    "* Tagging sentences with metadata, such as parts of speech (verbs, nouns, etc) or sentiment\n",
    "* Extracting information or computing statistics from text\n",
    "\n",
    "Compared to artificial languages like Python and XML, it's much more difficult to extract information from natural languages. NLP is a wide field; we only have time to learn the absolute basics. If you want to learn more, consider reading the entire [Natural Language Processing with Python][nlpp] book or taking a class in computational linguistics.\n",
    "\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "\n",
    "\n",
    "#### The Python NLP Ecosystem\n",
    "\n",
    "There are lots of Python packages for NLP (try searching online)! A few popular ones are:\n",
    "\n",
    "* [Natural Language Tool Kit][nltk] (`nltk`) is the most popular. It's designed for learning and research, so it's well-documented and has lots of features. We will use `nltk` for this class. \n",
    "* [TextBlob][textblob] is a \"simplified\" package. It has a nicer interface than NLTK, but less features.\n",
    "* [SpaCy][spacy] is a \"production-ready\" package, and the fastest of all the packages listed here. Useful for working with large natural language datasets.\n",
    "* [gensim][gensim] is a package for creating topic models, which are a kind of statistical model that predict the topics of a text.\n",
    "\n",
    "We're going to learn `nltk`, but you might want to try some of the others if your project involves NLP.\n",
    "\n",
    "[Stanford's Core NLP][CoreNLP] library is at the cutting edge of NLP research. It's developed in Java, but several Python packages provide an interface (such as [pynlp][] and [stanford-corenlp][]).\n",
    "\n",
    "[nltk]: https://www.nltk.org/\n",
    "[spacy]: https://spacy.io/\n",
    "[textblob]: https://textblob.readthedocs.io/en/dev/\n",
    "[gensim]: https://radimrehurek.com/gensim/\n",
    "[CoreNLP]: https://stanfordnlp.github.io/CoreNLP/\n",
    "[pynlp]: https://github.com/sina-al/pynlp\n",
    "[stanford-corenlp]: https://github.com/Lynten/stanford-corenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8d9a1",
   "metadata": {},
   "source": [
    "#### Corpora and Documents\n",
    "\n",
    "A _document_ is a single body text. When working with natural language data, documents are the unit of observation.\n",
    "\n",
    "What you choose as a document depends on the purpose of your analysis. If you're studying how people react to news on Twitter, it makes sense to use individual tweets as documents. If you're studying how animals are portrayed in 19th-century literature, you could use individual novels as documents.\n",
    "\n",
    "A _corpus_ is a collection of documents. In other words, a corpus is a dataset.\n",
    "\n",
    "`nltk` provides some example corpora in the `nltk.corpus` submodule. The documentation gives a [complete list](http://www.nltk.org/nltk_data/). Most have to be downloaded with `nltk.download()` before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ee052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a85790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download books from Project Gutenberg\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc7ea27",
   "metadata": {},
   "source": [
    "The `.fileids()` method lists the documents in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ebf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32a775",
   "metadata": {},
   "source": [
    "Lets talk about [whales](https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2H_4_0002). The `.raw()` method returns the raw text for a single document. Specify the document by its file ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf42b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby = nltk.corpus.gutenberg.raw(\"melville-moby_dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6402cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby[0:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ff79e",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "A _token_ is a sequence of characters to be treated as a group. Tokens are the unit of analysis for an indvidual document.\n",
    "\n",
    "Tokens can represent paragraphs, sentences, words, or something else. Most of the time, tokens will be words.\n",
    "\n",
    "When you analyze a document, the first step will usually be to split the document into tokens. Functions that do this are called _tokenizers_, and this process is called _tokenization_.\n",
    "\n",
    "The `nltk.sent_tokenize()` function splits a document into sentences, and the `nltk.word_tokenize()` function splits a document into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nltk.sent_tokenize(moby))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.sent_tokenize(moby)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.sent_tokenize(moby)[283]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(moby)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430eccfa",
   "metadata": {},
   "source": [
    "Corpora also have `.sents()` and `.word()` methods for tokenization. These methods are specialized to the corpus, so they sometimes use the different strategies than `sent_tokenize()` and `word_tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf09373",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = nltk.corpus.gutenberg.sents(\"melville-moby_dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300dbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffcba7d",
   "metadata": {},
   "source": [
    "### Strings and String Methods\n",
    "\n",
    "Lets continue talking about \t&#128011;. How does word tokenization actually work? The simplest strategy is to split at whitespace. You can do this with Python's built-in string methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby.split()[:10] # splits on whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb65913",
   "metadata": {},
   "source": [
    "Splitting on whitespace doesn't handle punctuation. You can use regular expressions to split on more complex patterns. Python's built-in `re` module provides regular expression functions [here](https://docs.python.org/3/library/re.html).\n",
    "\n",
    "```\n",
    "re.split(pattern, string, maxsplit=0, flags=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c530da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c23329",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"[ ',:]\", '[Moby Dick by Herman Melville 1851]\\r\\n\\r\\n\\r\\nETYMOLOGY.\\r\\n\\r\\n(Supplied by a Late Consumptive Usher to a Gr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b0815",
   "metadata": {},
   "source": [
    "What if we also want to split at newlines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815199c4",
   "metadata": {},
   "source": [
    "### Escape Sequences and Raw Strings\n",
    "\n",
    "In Python strings, backslash `\\` marks the beginning of an _escape sequence_. Escape sequences are special codes for writing characters that you can't otherwise type. For example, `\\n` is a new line character and `\\t` is a tab character.\n",
    "\n",
    "Since `\\` has a special meaning in strings, to write a literal `\\` you must use the escape sequence `\\\\`.\n",
    "\n",
    "You can see the actual characters in a string by printing the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a93eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\\n\\t\\\\world.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8c1ce",
   "metadata": {},
   "source": [
    "The regular expression (Regex) language is independent of Python and also uses backslash `\\` to mark the beginning of an escape sequence. Regex escape sequences disable special behavior for characters. For example, `.` matches any character, but `\\.` only matches a literal `.`.\n",
    "\n",
    "As a result, writing a regular expression in an ordinary Python string is awkward. For example, to match a literal `\\`, we need to write `\\\\` in regular expressions, which is `\\\\\\\\` in an ordinary Python string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b7928",
   "metadata": {},
   "source": [
    "Python provides _raw strings_, where `\\` has no special meaning for Python, to help solve this problem. You can create a raw string by putting an `r` before the starting quote:\n",
    "\n",
    "More about raw strings: [here](https://www.journaldev.com/23598/python-raw-string#:~:text=Python%20raw%20string%20is%20created,treated%20as%20an%20escape%20character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae96db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"\\\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b65ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r'\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857237b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Hi\\nHello'\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_s = r'Hi\\nHello'\n",
    "raw_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f11641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6119e",
   "metadata": {},
   "source": [
    "Even raw strings can't end in `\\;` this is a limitation of the Python parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150ef47",
   "metadata": {},
   "source": [
    "Now we can write a better regular expression to split with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ddcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    " moby[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"[\\.,:;\\r\\n\\t ()\\[\\]]\", moby[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(re.split(r\"[ \\[\\](),.:;!?'\\n\\r]\", moby))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(re.split(r\" \", moby))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1f16a6",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd76f3c",
   "metadata": {},
   "source": [
    "The regular expressions language includes _character classes_ that describe common sets of characters. The whitespace class `\\s` and the word class `\\w` are useful here (see [Reference](https://docs.python.org/3/library/re.html)). So to split on any whitespace character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = moby[:200]\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c55ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"\\w+\", string) #? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89054569",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"\\W*\", string) #? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05177d1b",
   "metadata": {},
   "source": [
    "In a raw string, `re.split` looks for regex escapes; in a non-raw string, the function looks for the literal ASCII character. If these coincide, the string does not have to be converted to a raw string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8556bca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.split(r\"[ \\[\\],.:;!'()\\n\\r-]\", moby) # note the '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"[ \\[\\],.:;!'()\\n]\", moby) # note the '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"[ \\[\\],.:;!'()\\n]\", moby) # note the '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19085e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"[\\s\\[\\],.:;!'()-]\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448bc59",
   "metadata": {},
   "source": [
    "Capitalizing a character classes inverts the meaning, so to split on all non-word characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"\\W+\", moby[:100]) # + matches 1 or more of the preceding characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3ff10",
   "metadata": {},
   "source": [
    "`\\w` means _any word character_\n",
    "\n",
    "`+` Causes the resulting RE to match 1 or more repetitions of the preceding RE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062eee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"\\W+\", \"the...dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"\\W+\", \"the,dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"\\W+\", \"the,I:! dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"\\W+\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2813cb77",
   "metadata": {},
   "source": [
    "Rather than splitting the text, you can also approach the problem from the perspective of extracting tokens. The `findall()` function returns all matches for a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd7c1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r\"\\w+\", \"The dog barked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\w\") # \\w is not a special python escape sequence, so it passes through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ffca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"\\W+\", \"The dog barked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362de51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"\\w+'?\\w*\", \"The' dog'ss toy barked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"[\\w']+\", \"I think the dog's toy barked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a0fb6",
   "metadata": {},
   "source": [
    "- `r\" \"`: read the string\n",
    "- `()+`: the patterns inside the parathesis should appear once or more\n",
    "- `\\w+`: the whole word\n",
    "- `|`: or\n",
    "\n",
    "More practice? [here](https://regex101.com/?fbclid=IwAR36UyAxywvpSvTOh7F-KYI72IZAVQ0wRcBc0OEOu6h4MifEf-iLcFedfyk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.findall(r\"\\w+\", moby)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f131f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby[34400:34530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(moby[34480:22500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby.find('CHAPTER 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda91b79",
   "metadata": {},
   "source": [
    "Lets try to match all chapters in the book. First, lets match the chapter sequence, they are similar to \"\\nCHAPTER 1\\r\\n\\r\\nLoomings.\\r\\n\". Check the novel [here](https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2H_4_0002). Note that the full stop after the chapter is not in the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79fdef0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "re.findall(r\"CHAPTER \\d+\\s+.*[\\.\\?\\!]\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2eafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(CHAPTER\\s{1}\\d+)\\s*(\\w+\\.{1})\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d16757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(moby[322415:322615])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31f630",
   "metadata": {},
   "source": [
    "See chapter 43. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa0543",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"CHAPTER \\d+\\s*.+[\\.!\\?]\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045fb1e",
   "metadata": {},
   "source": [
    "Chapter 1 reappeared! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4356ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s)(CHAPTER\\s{1}\\d+)\\s*(.+[\\.{1}|!{1}|\\?{1}])\", moby) # do not capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc0210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a80ba3ee",
   "metadata": {},
   "source": [
    "Lets use a negatve lookbehind! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(CHAPTER\\s{1}\\d+)\\s*(.+[\\.{1}|!{1}])\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac05ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r\"CHAPTER (\\d+)\\s+.+[\\.!\\?]\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06f461",
   "metadata": {},
   "source": [
    "Lets find the unmatched chapters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ce5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chapters = [i for i in range(1,136)]\n",
    "matched_chapters = [int(i) for i in \n",
    "                    re.findall(r\"CHAPTER (\\d+)\\s+.+[\\.!\\?]\", moby)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.findall(r\"CHAPTER (\\d+)\\s+.+[\\.!\\?]\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in all_chapters if not i in matched_chapters ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fde6b8",
   "metadata": {},
   "source": [
    "There is another new line! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c752b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(CHAPTER \\d+\\s+.+\\n.+[\\.!\\?])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c19133",
   "metadata": {},
   "source": [
    "Lets be lazy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa926cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(CHAPTER\\s{1}\\d+)\\s*(.+?\\s*.*[\\.{1}|!{1}|\\?{1}])\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac30e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b86c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51836910",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'''((CHAPTER \\d+|Epilogue)\\s+.+\\n.+[\\.!\\?])''', moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03d173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed363f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733c21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS)\\s*(.+?\\s*.*[\\.{1}|!{1}|\\?{1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a2e15",
   "metadata": {},
   "source": [
    "Lets use a positive lookahead `(?=...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS(?=\\s*\\())\\s*(.+?\\s*.*[\\.{1}|!{1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cb354",
   "metadata": {},
   "source": [
    "To match `\"ETYMOLOGY.\"`, we have to account for parenthesis. (Note the extra `\\.*`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27eb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(ETYMOLOGY|CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS(?=\\s*\\())\\.*\\s*(.+?\\s*.*[\\.{1}|!{1}|\\){1}|\\?{1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20bf205",
   "metadata": {},
   "source": [
    "Perfect! But what if we want to match the chapters that follow after his matched string? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"((?<!,\\s{1})(?:ETYMOLOGY|CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS(?=\\s*\\())(?:\\.*\\s*).+?\\s*.*[\\.{1}|!{1}|\\){1}|\\?{1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb2cfa",
   "metadata": {},
   "source": [
    "Check the [docs](https://docs.python.org/3/library/re.html#re.split). Remove the capturing group when splitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d76beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = re.split(r\"(?<!,\\s{1})(?:ETYMOLOGY|CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS(?=\\s*\\())(?:\\.*\\s*).+?\\s*.*[\\.{1}|!{1}|\\){1}|\\?{1}]\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = [re.sub(r\"\\s+\", \" \", chapter) for chapter in chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f08fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter = chapters[3]\n",
    "chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a240dc91",
   "metadata": {},
   "source": [
    "Back to tokenizing! Tokenizing natural languages is a difficult problem. Some tokenizers work better for certain kinds of documents than others.\n",
    "\n",
    "Before building your own tokenizer, try the tokenizers included with __nltk__, in the `nltk.tokenize` submodule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2b12ec50b0a525a62abe739e766b0c808eccd181e3f804cedbbca00f4d5b392"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
